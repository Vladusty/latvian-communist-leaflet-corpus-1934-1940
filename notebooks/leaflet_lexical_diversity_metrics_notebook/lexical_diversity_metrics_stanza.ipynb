{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy stanza\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KnteknKBFKv",
        "outputId": "36b4b446-7f3b-4b16-b808-2d05de2ec957"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2t_XCONBCOC",
        "outputId": "5bc9481d-92d2-4487-deb9-a55d4b236289"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                file  id  \\\n",
              " 0  revl-n001-LKP_LKJS_Vidienas_org-5000-[1934-01-...   1   \n",
              " 1  revl-n002-LKP_soldiers_org-1500-[1934-01-11…].txt   2   \n",
              " 2              revl-n003-SP_CK-unk-[…1934-01-21].txt   3   \n",
              " 3           revl-n004a-LKP_CK-3000-[…1934-01-30].txt  4a   \n",
              " 4          revl-n004b-LKP_CK-10000-[…1934-01-30].txt  4b   \n",
              " \n",
              "                                                title  \\\n",
              " 0  LKP un LKJS Vidienas organizācijas lapiņa par ...   \n",
              " 1  No LKP kareivju organizācijas lapiņas ar karei...   \n",
              " 2  Latvijas Sarkanās Palīdzības CK lapiņa, kas ve...   \n",
              " 3  LKP CK aicinājums bezdarbniekiem organizēties ...   \n",
              " 4  LKP CK aicinājums bezdarbniekiem organizēties ...   \n",
              " \n",
              "                                               author           date print_run  \\\n",
              " 0  LKP Vidienas organizācija un LKJS Vidienas org...  [1934-01-11…]      5000   \n",
              " 1                          LKP kareivju organizācija  [1934-01-11…]      1500   \n",
              " 2     Latvijas Sarkanās Palīdzības Centrālā komiteja  […1934-01-21]         ?   \n",
              " 3                                             LKP CK  […1934-01-30]      3000   \n",
              " 4                                             LKP CK        1934-02     10000   \n",
              " \n",
              "                    typography_name  N_tokens  V_lemmas       TTR       RTTR  \\\n",
              " 0                         Spartaks       463       278  0.600432  12.919756   \n",
              " 1                         Spartaks       310       207  0.667742  11.756810   \n",
              " 2  «Sarkanās Palīdzības» spiestuve       712       339  0.476124  12.704562   \n",
              " 3                         Spartaks       945       412  0.435979  13.402361   \n",
              " 4                         Spartaks       945       412  0.435979  13.402361   \n",
              " \n",
              "        CTTR  Herdan_C   Maas_a2  Hapax  HapaxShare  \n",
              " 0  9.135647  0.916890  0.013541    206    0.741007  \n",
              " 1  8.313320  0.929600  0.012272    161    0.777778  \n",
              " 2  8.983482  0.887018  0.017202    209    0.616519  \n",
              " 3  9.476900  0.878829  0.017686    259    0.628641  \n",
              " 4  9.476900  0.878829  0.017686    259    0.628641  ,\n",
              "                                           scope  documents  N_tokens  \\\n",
              " 0  Latvian Communist Leaflet Corpus (1934–1940)        266    156714   \n",
              " \n",
              "    V_lemmas       TTR       RTTR      CTTR  Herdan_C   Maas_a2  Hapax  \\\n",
              " 0     12461  0.079514  31.477411  22.25789  0.788348  0.017693   5772   \n",
              " \n",
              "    HapaxShare  \n",
              " 0    0.463205  )"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import stanza\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "\n",
        "ZIP_PATH = \"/content/latvian_communist_leaflets_1934-1940.zip\"\n",
        "BASE_DIR = Path(\"/content/leaflets_unzipped\")\n",
        "\n",
        "BASE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "    z.extractall(BASE_DIR)\n",
        "\n",
        "# NLP pipeline\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"lv\",\n",
        "    processors=\"tokenize,lemma\",\n",
        "    tokenize_no_ssplit=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# HELPERS\n",
        "# =========================\n",
        "\n",
        "def extract_leaflet_text(raw: str) -> str:\n",
        "    m = re.search(r\"\\btext\\s*::?\\s*(.*)\\Z\", raw, flags=re.IGNORECASE | re.DOTALL)\n",
        "    return m.group(1).strip() if m else raw.strip()\n",
        "\n",
        "def lemmatize_lv(text: str):\n",
        "    doc = nlp(text)\n",
        "    return [\n",
        "        w.lemma.lower()\n",
        "        for sent in doc.sentences\n",
        "        for w in sent.words\n",
        "        if w.text.isalpha()\n",
        "    ]\n",
        "\n",
        "def parse_metadata(raw: str):\n",
        "    def get(key):\n",
        "        m = re.search(rf\"^\\s*{re.escape(key)}\\s*:\\s*(.+)\\s*$\",\n",
        "                      raw, flags=re.IGNORECASE | re.MULTILINE)\n",
        "        return m.group(1).strip() if m else None\n",
        "    return {\n",
        "        \"id\": get(\"id\"),\n",
        "        \"title\": get(\"title\"),\n",
        "        \"author\": get(\"author\"),\n",
        "        \"date\": get(\"date\"),\n",
        "        \"print_run\": get(\"print_run\"),\n",
        "        \"typography_name\": get(\"typography_name\"),\n",
        "    }\n",
        "\n",
        "def lexical_diversity(tokens):\n",
        "    N = len(tokens)\n",
        "    if N == 0:\n",
        "        return {}\n",
        "\n",
        "    freq = Counter(tokens)\n",
        "    V = len(freq)\n",
        "    hapax = sum(1 for c in freq.values() if c == 1)\n",
        "\n",
        "    logN = math.log(N)\n",
        "    logV = math.log(V)\n",
        "\n",
        "    return {\n",
        "        \"N_tokens\": N,\n",
        "        \"V_lemmas\": V,\n",
        "        \"TTR\": V / N,\n",
        "        \"RTTR\": V / math.sqrt(N),\n",
        "        \"CTTR\": V / math.sqrt(2 * N),\n",
        "        \"Herdan_C\": logV / logN,\n",
        "        \"Maas_a2\": (logN - logV) / (logN ** 2),\n",
        "        \"Hapax\": hapax,\n",
        "        \"HapaxShare\": hapax / V\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# PROCESS CORPUS\n",
        "# =========================\n",
        "\n",
        "rows = []\n",
        "all_lemmas = []\n",
        "\n",
        "txt_files = sorted(BASE_DIR.rglob(\"*.txt\"))\n",
        "if not txt_files:\n",
        "    raise RuntimeError(\"В ZIP нет .txt файлов\")\n",
        "\n",
        "for fp in txt_files:\n",
        "    raw = fp.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "    text = extract_leaflet_text(raw)\n",
        "    lemmas = lemmatize_lv(text)\n",
        "\n",
        "    meta = parse_metadata(raw)\n",
        "    metrics = lexical_diversity(lemmas)\n",
        "\n",
        "    rows.append({\n",
        "        \"file\": fp.name,\n",
        "        **meta,\n",
        "        **metrics\n",
        "    })\n",
        "\n",
        "    all_lemmas.extend(lemmas)\n",
        "\n",
        "df_docs = pd.DataFrame(rows)\n",
        "\n",
        "# =========================\n",
        "# CORPUS-LEVEL METRICS\n",
        "# =========================\n",
        "\n",
        "corpus_metrics = lexical_diversity(all_lemmas)\n",
        "df_corpus = pd.DataFrame([{\n",
        "    \"scope\": \"Latvian Communist Leaflet Corpus (1934–1940)\",\n",
        "    \"documents\": len(df_docs),\n",
        "    **corpus_metrics\n",
        "}])\n",
        "\n",
        "# =========================\n",
        "# SAVE\n",
        "# =========================\n",
        "\n",
        "df_docs.to_csv(\"/content/lexdiv_by_leaflet_lemmatized.csv\", index=False)\n",
        "df_corpus.to_csv(\"/content/lexdiv_corpus_lemmatized.csv\", index=False)\n",
        "\n",
        "df_docs.head(), df_corpus\n"
      ]
    }
  ]
}