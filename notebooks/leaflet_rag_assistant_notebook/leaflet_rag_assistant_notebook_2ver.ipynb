{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio sentence-transformers faiss-cpu requests rank-bm25"
      ],
      "metadata": {
        "id": "cmpiJhBKDUQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f48e49-7e56-461a-f84c-d70959fbf786"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "1LolAprIPTTh",
        "outputId": "eed09950-4fde-4996-f935-8b1e2a8bf4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://62f3448933d042e20a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62f3448933d042e20a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tempfile\n",
        "from typing import List, Dict, Callable, Tuple, Optional\n",
        "from pathlib import Path\n",
        "from datetime import date, datetime\n",
        "import calendar\n",
        "import json\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# BM25 tokenizer (noņem diakritiku NEVAJAG; saglabā LV burtus)\n",
        "# ============================================================\n",
        "_TOKEN_RE = re.compile(r\"[0-9A-Za-zĀāČčĒēĢģĪīĶķĻļŅņŠšŪūŽž]+\", re.UNICODE)\n",
        "\n",
        "def bm25_tokenize(text: str) -> List[str]:\n",
        "    return _TOKEN_RE.findall((text or \"\").lower())\n",
        "\n",
        "\n",
        "AUTO_MODE = \"AUTO\"\n",
        "MANUAL_MODE = \"MANUAL\"\n",
        "\n",
        "def sem_percent_to_alpha(sem_percent: float) -> float:\n",
        "    p = float(sem_percent)\n",
        "    if p < 0:\n",
        "        p = 0.0\n",
        "    if p > 100:\n",
        "        p = 100.0\n",
        "    return p / 100.0\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# OpenRouter\n",
        "# ===========================\n",
        "\n",
        "DEFAULT_MODEL_ID = \"meta-llama/llama-3.3-70b-instruct:free\" # \"anthropic/claude-3.5-sonnet\"\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "LOG_PATH = \"lkp_rag_log.json\"\n",
        "\n",
        "\n",
        "def ask(\n",
        "    prompt: str,\n",
        "    api_key: str,\n",
        "    model_id: Optional[str] = None,\n",
        "    temperature: float = 0.0,\n",
        ") -> str:\n",
        "    if not api_key.strip():\n",
        "        raise RuntimeError(\"OpenRouter API key nav norādīts.\")\n",
        "\n",
        "    model_id = (model_id or DEFAULT_MODEL_ID).strip()\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key.strip()}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"Tu esi precīzs, kritisks vēsturnieks, kas analizē \"\n",
        "                    \"Latvijas komunistisko pagrīdes organizāciju skrejlapas (1934–1940).\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        \"temperature\": float(temperature),\n",
        "    }\n",
        "\n",
        "    resp = requests.post(OPENROUTER_URL, headers=headers, json=data, timeout=120)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# JSON LOG (NEW FORMAT)\n",
        "# ===========================\n",
        "def _serialize_chunk_for_log(chunk: Dict) -> Dict:\n",
        "    # STRICTLY pēc tavas BM25 programmas: pilns teksts + score_semantic/score_bm25\n",
        "    return {\n",
        "        \"doc_id\": chunk.get(\"doc_id\"),\n",
        "        \"file_name\": chunk.get(\"file_name\"),\n",
        "        \"title\": chunk.get(\"title\"),\n",
        "        \"chunk_id\": chunk.get(\"chunk_id\"),\n",
        "        \"score\": chunk.get(\"score\"),\n",
        "        \"score_semantic\": chunk.get(\"score_semantic\"),\n",
        "        \"score_bm25\": chunk.get(\"score_bm25\"),\n",
        "        \"text\": chunk.get(\"text\", \"\") or \"\",\n",
        "    }\n",
        "\n",
        "\n",
        "def log_qa_event(\n",
        "    question: str,\n",
        "    answer: str,\n",
        "    retrieved_chunks: List[Dict],\n",
        "    model_id: str,\n",
        "    top_k: int,\n",
        "    temperature: float,\n",
        "    query_profile: Optional[\"QueryProfile\"] = None,\n",
        "    weight_info: Optional[Dict] = None,\n",
        ") -> None:\n",
        "    event = {\n",
        "        \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"model_id\": model_id,\n",
        "        \"top_k\": int(top_k),\n",
        "        \"temperature\": float(temperature),\n",
        "\n",
        "        \"query_profile\": None if query_profile is None else {\n",
        "            \"qtype\": query_profile.qtype,\n",
        "            \"alpha_auto_suggested\": float(query_profile.alpha),\n",
        "            \"alpha_effective\": None if weight_info is None else float(weight_info[\"alpha_semantic\"]),\n",
        "            \"sem_candidates\": int(query_profile.sem_candidates),\n",
        "            \"bm25_candidates\": int(query_profile.bm25_candidates),\n",
        "        },\n",
        "\n",
        "        \"retrieval_weights\": weight_info,\n",
        "\n",
        "        \"retrieved_chunks\": [\n",
        "            _serialize_chunk_for_log(c) for c in (retrieved_chunks or [])\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(LOG_PATH):\n",
        "            try:\n",
        "                with open(LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read().strip()\n",
        "                    data = json.loads(content) if content else []\n",
        "            except Exception:\n",
        "                data = []\n",
        "        else:\n",
        "            data = []\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            data = [data]\n",
        "\n",
        "        data.append(event)\n",
        "\n",
        "        with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not write log: {e}\")\n",
        "\n",
        "\n",
        "def get_log_file_for_gui():\n",
        "    if not os.path.exists(LOG_PATH):\n",
        "        with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump([], f, ensure_ascii=False, indent=2)\n",
        "    return LOG_PATH\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1) Datumi / tirāža (no leaflet programmas)\n",
        "# ============================================================\n",
        "def _parse_plain_date_to_range(s: str) -> Tuple[Optional[date], Optional[date]]:\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return None, None\n",
        "\n",
        "    parts = s.split(\"-\")\n",
        "    try:\n",
        "        if len(parts) == 3:\n",
        "            y, m, d = map(int, parts)\n",
        "            dt = date(y, m, d)\n",
        "            return dt, dt\n",
        "        elif len(parts) == 2:\n",
        "            y, m = map(int, parts)\n",
        "            last_day = calendar.monthrange(y, m)[1]\n",
        "            return date(y, m, 1), date(y, m, last_day)\n",
        "        else:\n",
        "            y = int(parts[0])\n",
        "            return date(y, 1, 1), date(y, 12, 31)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def parse_metadata_date_to_range(date_str: str) -> Tuple[Optional[date], Optional[date]]:\n",
        "    if not date_str:\n",
        "        return None, None\n",
        "\n",
        "    date_str = date_str.strip()\n",
        "\n",
        "    if not (date_str.startswith(\"[\") and date_str.endswith(\"]\")):\n",
        "        return _parse_plain_date_to_range(date_str)\n",
        "\n",
        "    inner = date_str[1:-1].strip()\n",
        "\n",
        "    if \"...\" in inner:\n",
        "        if inner.endswith(\"...\"):\n",
        "            left = inner[:-3].strip()\n",
        "            start, end = _parse_plain_date_to_range(left)\n",
        "            return start, None\n",
        "        elif inner.startswith(\"...\"):\n",
        "            right = inner[3:].strip()\n",
        "            start, end = _parse_plain_date_to_range(right)\n",
        "            return None, end\n",
        "\n",
        "    if \"..\" in inner:\n",
        "        left, right = inner.split(\"..\", 1)\n",
        "        left = left.strip()\n",
        "        right = right.strip()\n",
        "        s1, e1 = _parse_plain_date_to_range(left)\n",
        "        s2, e2 = _parse_plain_date_to_range(right)\n",
        "        start = s1\n",
        "        end = e2\n",
        "        return start, end\n",
        "\n",
        "    return _parse_plain_date_to_range(inner)\n",
        "\n",
        "\n",
        "def parse_print_run_value(v: str) -> Optional[int]:\n",
        "    if not v:\n",
        "        return None\n",
        "    v_low = v.strip().lower()\n",
        "    if v_low == \"unk\":\n",
        "        return None\n",
        "    digits = \"\".join(ch for ch in v_low if ch.isdigit())\n",
        "    if not digits:\n",
        "        return None\n",
        "    try:\n",
        "        return int(digits)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_user_date_box(s: str, is_start: bool) -> Optional[date]:\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return None\n",
        "    parts = s.split(\"-\")\n",
        "    if len(parts) == 3:\n",
        "        y, m, d = map(int, parts)\n",
        "        return date(y, m, d)\n",
        "    elif len(parts) == 2:\n",
        "        y, m = map(int, parts)\n",
        "        if is_start:\n",
        "            return date(y, m, 1)\n",
        "        else:\n",
        "            last_day = calendar.monthrange(y, m)[1]\n",
        "            return date(y, m, last_day)\n",
        "    else:\n",
        "        y = int(parts[0])\n",
        "        return date(y, 1, 1) if is_start else date(y, 12, 31)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1b) Loading leaflets from ZIP (no leaflet programmas)\n",
        "# ============================================================\n",
        "def parse_metadata(content: str) -> Dict:\n",
        "    parts = content.split(\"text:\", 1)\n",
        "    metadata_text = parts[0]\n",
        "\n",
        "    metadata: Dict = {\n",
        "        \"id\": None,\n",
        "        \"file_name\": \"\",\n",
        "        \"title\": \"\",\n",
        "        \"author\": \"\",\n",
        "        \"date\": \"\",\n",
        "        \"print_run\": \"\",\n",
        "        \"typography_name\": \"\",\n",
        "        \"source\": \"\",\n",
        "        \"text\": \"\",\n",
        "    }\n",
        "\n",
        "    for line in metadata_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if \":\" in line:\n",
        "            key, value = line.split(\":\", 1)\n",
        "            key = key.strip()\n",
        "            value = value.strip()\n",
        "\n",
        "            if key == \"id\":\n",
        "                try:\n",
        "                    metadata[key] = int(value)\n",
        "                except ValueError:\n",
        "                    metadata[key] = None\n",
        "            elif key in metadata:\n",
        "                metadata[key] = value\n",
        "\n",
        "    if len(parts) > 1:\n",
        "        metadata[\"text\"] = parts[1].strip()\n",
        "\n",
        "    raw_date = metadata.get(\"date\", \"\")\n",
        "    try:\n",
        "        d_start, d_end = parse_metadata_date_to_range(raw_date)\n",
        "    except Exception:\n",
        "        d_start, d_end = None, None\n",
        "\n",
        "    metadata[\"date_start\"] = d_start\n",
        "    metadata[\"date_end\"] = d_end\n",
        "    metadata[\"print_run_value\"] = parse_print_run_value(metadata.get(\"print_run\", \"\"))\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_leaflets_from_zip(zip_path: str) -> List[Dict]:\n",
        "    results: List[Dict] = []\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(temp_dir)\n",
        "\n",
        "        corpus_dir: Optional[str] = None\n",
        "        if any(Path(temp_dir).glob(\"*.txt\")):\n",
        "            corpus_dir = temp_dir\n",
        "        else:\n",
        "            for item in os.listdir(temp_dir):\n",
        "                potential_corpus_dir = os.path.join(temp_dir, item)\n",
        "                if os.path.isdir(potential_corpus_dir) and any(\n",
        "                    Path(potential_corpus_dir).glob(\"*.txt\")\n",
        "                ):\n",
        "                    corpus_dir = potential_corpus_dir\n",
        "                    break\n",
        "\n",
        "        if not corpus_dir:\n",
        "            raise ValueError(\"Cannot find corpus directory with .txt files in ZIP file\")\n",
        "\n",
        "        for file_path in Path(corpus_dir).glob(\"*.txt\"):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                leaflet_data = parse_metadata(content)\n",
        "                leaflet_data[\"path\"] = str(file_path)\n",
        "                if not leaflet_data.get(\"file_name\"):\n",
        "                    leaflet_data[\"file_name\"] = file_path.name\n",
        "                if not leaflet_data.get(\"title\"):\n",
        "                    leaflet_data[\"title\"] = file_path.stem\n",
        "\n",
        "                results.append(leaflet_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Chunking\n",
        "# ============================================================\n",
        "def chunk_text(text: str, max_words: int = 300, overlap_words: int = 60) -> List[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    if overlap_words < 0:\n",
        "        overlap_words = 0\n",
        "    if overlap_words >= max_words:\n",
        "        overlap_words = max_words // 3\n",
        "\n",
        "    chunks: List[str] = []\n",
        "    step = max_words - overlap_words\n",
        "\n",
        "    start = 0\n",
        "    n = len(words)\n",
        "    while start < n:\n",
        "        end = min(start + max_words, n)\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append(chunk)\n",
        "        if end == n:\n",
        "            break\n",
        "        start += step\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Filters (no leaflet programmas)\n",
        "# ============================================================\n",
        "def chunk_matches_filters(chunk: Dict, filters: Dict) -> bool:\n",
        "    # Date filter\n",
        "    df = filters.get(\"date_from\")\n",
        "    dt = filters.get(\"date_to\")\n",
        "    if df or dt:\n",
        "        s = chunk.get(\"date_start\")\n",
        "        e = chunk.get(\"date_end\")\n",
        "        if s is None and e is None:\n",
        "            return False\n",
        "        if df and e is not None and e < df:\n",
        "            return False\n",
        "        if dt and s is not None and s > dt:\n",
        "            return False\n",
        "\n",
        "    # Print run filter\n",
        "    pr_min = filters.get(\"print_run_min\")\n",
        "    pr_max = filters.get(\"print_run_max\")\n",
        "    include_unk = bool(filters.get(\"include_unk_print_run\", False))\n",
        "\n",
        "    if pr_min is not None or pr_max is not None:\n",
        "        pr = chunk.get(\"print_run_value\")\n",
        "        if pr is None:\n",
        "            if not include_unk:\n",
        "                return False\n",
        "        else:\n",
        "            if pr_min is not None and pr < pr_min:\n",
        "                return False\n",
        "            if pr_max is not None and pr > pr_max:\n",
        "                return False\n",
        "\n",
        "    # Org substring filter (author+source+title+file_name)\n",
        "    org_subs = filters.get(\"org_substrings\") or []\n",
        "    if org_subs:\n",
        "        org_meta = (\n",
        "            (chunk.get(\"author\", \"\") + \" \" +\n",
        "             chunk.get(\"source\", \"\") + \" \" +\n",
        "             chunk.get(\"title\", \"\") + \" \" +\n",
        "             chunk.get(\"file_name\", \"\"))\n",
        "            .lower()\n",
        "        )\n",
        "        if not any(sub in org_meta for sub in org_subs):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# QueryProfile (no BM25 programmas)\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class QueryProfile:\n",
        "    qtype: str\n",
        "    alpha: float\n",
        "    sem_candidates: int\n",
        "    bm25_candidates: int\n",
        "\n",
        "\n",
        "_FACTOID_STARTS = {\n",
        "    \"kas\", \"kur\", \"kad\", \"cik\",\n",
        "    \"kāds\", \"kāda\", \"kādi\",\n",
        "    \"kurš\", \"kura\", \"kuru\", \"kuras\",\n",
        "}\n",
        "\n",
        "def _has_year_or_number(q: str) -> bool:\n",
        "    return bool(re.search(r\"\\b(\\d{3,4}|\\d+)\\b\", q))\n",
        "\n",
        "def _has_quotes(q: str) -> bool:\n",
        "    return any(ch in q for ch in ['\"', \"“\", \"”\", \"«\", \"»\"])\n",
        "\n",
        "\n",
        "def classify_query_rule_based(q: str, top_k: int) -> QueryProfile:\n",
        "    q0 = (q or \"\").strip().lower()\n",
        "    toks = bm25_tokenize(q0)\n",
        "    if not toks:\n",
        "        return QueryProfile(\n",
        "            qtype=\"general\",\n",
        "            alpha=0.75,\n",
        "            sem_candidates=max(top_k * 8, top_k),\n",
        "            bm25_candidates=max(top_k * 3, top_k),\n",
        "        )\n",
        "\n",
        "    first = toks[0]\n",
        "    n_tok = len(toks)\n",
        "\n",
        "    fact_signals = 0\n",
        "    if first in _FACTOID_STARTS:\n",
        "        fact_signals += 2\n",
        "    if _has_year_or_number(q0):\n",
        "        fact_signals += 2\n",
        "    if _has_quotes(q0):\n",
        "        fact_signals += 1\n",
        "    if n_tok <= 6:\n",
        "        fact_signals += 1\n",
        "\n",
        "    general_signals = 0\n",
        "    if any(w in toks for w in [\n",
        "        \"kāpēc\", \"kādēļ\", \"paskaidro\", \"apraksti\", \"analizē\", \"novērtē\",\n",
        "        \"raksturo\", \"salīdzini\", \"nozīme\", \"loma\", \"sekas\", \"iemesli\",\n",
        "    ]):\n",
        "        general_signals += 2\n",
        "    if n_tok >= 10:\n",
        "        general_signals += 1\n",
        "\n",
        "    if fact_signals >= general_signals + 1:\n",
        "        return QueryProfile(\n",
        "            qtype=\"factoid\",\n",
        "            alpha=0.40,\n",
        "            sem_candidates=max(top_k * 3, top_k),\n",
        "            bm25_candidates=max(top_k * 8, top_k),\n",
        "        )\n",
        "\n",
        "    return QueryProfile(\n",
        "        qtype=\"general\",\n",
        "        alpha=0.70,\n",
        "        sem_candidates=max(top_k * 8, top_k),\n",
        "        bm25_candidates=max(top_k * 3, top_k),\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# LeafletRAG HYBRID (SEM + BM25) + filters\n",
        "# ============================================================\n",
        "class LeafletRAGHybrid:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index: Optional[faiss.IndexFlatIP] = None\n",
        "        self.chunks: List[Dict] = []\n",
        "        self.embedding_dim: Optional[int] = None\n",
        "        self.bm25: Optional[BM25Okapi] = None\n",
        "        self.bm25_tokens: List[List[str]] = []\n",
        "\n",
        "    def build_index(\n",
        "        self,\n",
        "        leaflets: List[Dict],\n",
        "        max_words_per_chunk: int = 260,\n",
        "        overlap_words: int = 60,\n",
        "        normalize_embeddings: bool = True,\n",
        "    ) -> None:\n",
        "        all_texts: List[str] = []\n",
        "        self.chunks = []\n",
        "\n",
        "        for leaflet in leaflets:\n",
        "            full_text = leaflet.get(\"text\", \"\") or \"\"\n",
        "            if not full_text.strip():\n",
        "                continue\n",
        "\n",
        "            leaflet_id = leaflet.get(\"id\")\n",
        "            file_name = leaflet.get(\"file_name\", \"\")\n",
        "            title = leaflet.get(\"title\", \"\")\n",
        "            date_str = leaflet.get(\"date\", \"\")\n",
        "            date_start = leaflet.get(\"date_start\")\n",
        "            date_end = leaflet.get(\"date_end\")\n",
        "            print_run = leaflet.get(\"print_run\", \"\")\n",
        "            print_run_value = leaflet.get(\"print_run_value\")\n",
        "            author = leaflet.get(\"author\", \"\")\n",
        "            source = leaflet.get(\"source\", \"\")\n",
        "\n",
        "            chunk_list = chunk_text(full_text, max_words=max_words_per_chunk, overlap_words=overlap_words)\n",
        "            for i, chunk in enumerate(chunk_list):\n",
        "                # doc_id = leaflet_id (lai JSON laukos būtu doc_id, kā tu gribi)\n",
        "                self.chunks.append(\n",
        "                    {\n",
        "                        \"doc_id\": leaflet_id,\n",
        "                        \"leaflet_id\": leaflet_id,  # saglabājam arī oriģinālo (var noderēt)\n",
        "                        \"file_name\": file_name,\n",
        "                        \"title\": title,\n",
        "                        \"date\": date_str,\n",
        "                        \"date_start\": date_start,\n",
        "                        \"date_end\": date_end,\n",
        "                        \"print_run\": print_run,\n",
        "                        \"print_run_value\": print_run_value,\n",
        "                        \"author\": author,\n",
        "                        \"source\": source,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"text\": chunk,\n",
        "                    }\n",
        "                )\n",
        "                all_texts.append(chunk)\n",
        "\n",
        "        if not all_texts:\n",
        "            raise ValueError(\"No text chunks found. Cannot build index.\")\n",
        "\n",
        "        # BM25\n",
        "        self.bm25_tokens = [bm25_tokenize(t) for t in all_texts]\n",
        "        self.bm25 = BM25Okapi(self.bm25_tokens)\n",
        "\n",
        "        # SEM embeddings + FAISS\n",
        "        embeddings = self.model.encode(all_texts, convert_to_numpy=True)\n",
        "        if normalize_embeddings:\n",
        "            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12\n",
        "            embeddings = embeddings / norms\n",
        "\n",
        "        self.embedding_dim = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def retrieve(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 5,\n",
        "        filters: Optional[Dict] = None,\n",
        "        normalize_embeddings: bool = True,\n",
        "        sem_candidates: Optional[int] = None,\n",
        "        bm25_candidates: Optional[int] = None,\n",
        "        weight_mode: str = AUTO_MODE,\n",
        "        sem_percent: float = 40.0,\n",
        "    ) -> Tuple[List[Dict], QueryProfile, Dict]:\n",
        "        if self.index is None or not self.chunks:\n",
        "            raise RuntimeError(\"Index is not built. Call build_index() first.\")\n",
        "        if self.bm25 is None or not self.bm25_tokens:\n",
        "            raise RuntimeError(\"BM25 is not built. Check build_index().\")\n",
        "\n",
        "        if filters is None:\n",
        "            filters = {}\n",
        "\n",
        "        # AUTO profile vienmēr rēķinām (logam)\n",
        "        profile = classify_query_rule_based(query, top_k=top_k)\n",
        "\n",
        "        mode = (weight_mode or AUTO_MODE).upper().strip()\n",
        "        if mode == MANUAL_MODE:\n",
        "            alpha = sem_percent_to_alpha(sem_percent)\n",
        "        else:\n",
        "            alpha = profile.alpha\n",
        "\n",
        "        weight_info = {\n",
        "            \"mode\": \"manual\" if mode == MANUAL_MODE else \"auto\",\n",
        "            \"alpha_semantic\": float(alpha),\n",
        "            \"alpha_bm25\": float(1.0 - alpha),\n",
        "            \"sem_percent\": float(alpha * 100.0),\n",
        "            \"bm25_percent\": float((1.0 - alpha) * 100.0),\n",
        "        }\n",
        "\n",
        "        if sem_candidates is None:\n",
        "            sem_candidates = profile.sem_candidates\n",
        "        if bm25_candidates is None:\n",
        "            bm25_candidates = profile.bm25_candidates\n",
        "\n",
        "        # ---- SEM candidates (FAISS)\n",
        "        query_emb = self.model.encode([query], convert_to_numpy=True)\n",
        "        if normalize_embeddings:\n",
        "            norms = np.linalg.norm(query_emb, axis=1, keepdims=True) + 1e-12\n",
        "            query_emb = query_emb / norms\n",
        "\n",
        "        sem_scores, sem_indices = self.index.search(query_emb, min(sem_candidates, len(self.chunks)))\n",
        "        sem_scores = sem_scores[0]\n",
        "        sem_indices = sem_indices[0]\n",
        "\n",
        "        sem_map = {}\n",
        "        for s, idx in zip(sem_scores, sem_indices):\n",
        "            if 0 <= idx < len(self.chunks):\n",
        "                sem_map[int(idx)] = float(s)\n",
        "\n",
        "        # ---- BM25 candidates\n",
        "        q_tokens = bm25_tokenize(query)\n",
        "        bm25_all = self.bm25.get_scores(q_tokens)\n",
        "\n",
        "        if bm25_candidates >= len(bm25_all):\n",
        "            bm25_top_idx = np.argsort(-bm25_all)\n",
        "        else:\n",
        "            bm25_top_idx = np.argpartition(-bm25_all, bm25_candidates)[:bm25_candidates]\n",
        "            bm25_top_idx = bm25_top_idx[np.argsort(-bm25_all[bm25_top_idx])]\n",
        "\n",
        "        bm25_map = {int(i): float(bm25_all[i]) for i in bm25_top_idx}\n",
        "\n",
        "        # ---- candidates union\n",
        "        cand_idx = set(sem_map.keys()) | set(bm25_map.keys())\n",
        "        if not cand_idx:\n",
        "            return [], profile, weight_info\n",
        "\n",
        "        # ---- normalize within candidate set\n",
        "        sem_vals = [sem_map.get(i, 0.0) for i in cand_idx]\n",
        "        bm_vals = [bm25_map.get(i, 0.0) for i in cand_idx]\n",
        "        sem_min, sem_max = min(sem_vals), max(sem_vals)\n",
        "        bm_min, bm_max = min(bm_vals), max(bm_vals)\n",
        "\n",
        "        def norm(x, a, b):\n",
        "            if b - a < 1e-12:\n",
        "                return 0.0\n",
        "            return (x - a) / (b - a)\n",
        "\n",
        "        scored = []\n",
        "        for i in cand_idx:\n",
        "            s_sem = sem_map.get(i, 0.0)\n",
        "            s_bm = bm25_map.get(i, 0.0)\n",
        "            s_sem_n = norm(s_sem, sem_min, sem_max)\n",
        "            s_bm_n = norm(s_bm, bm_min, bm_max)\n",
        "            combo = alpha * s_sem_n + (1.0 - alpha) * s_bm_n\n",
        "            scored.append((combo, i, s_sem, s_bm))\n",
        "\n",
        "        scored.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "        results: List[Dict] = []\n",
        "        for combo, idx, s_sem, s_bm in scored:\n",
        "            if idx < 0 or idx >= len(self.chunks):\n",
        "                continue\n",
        "\n",
        "            chunk_info = self.chunks[idx].copy()\n",
        "            chunk_info[\"score\"] = float(combo)\n",
        "            chunk_info[\"score_semantic\"] = float(s_sem)\n",
        "            chunk_info[\"score_bm25\"] = float(s_bm)\n",
        "\n",
        "            # IMPORTANT: leaflet filters tieši šeit (tātad ietekmē retrieval rezultātu)\n",
        "            if not chunk_matches_filters(chunk_info, filters):\n",
        "                continue\n",
        "\n",
        "            results.append(chunk_info)\n",
        "            if len(results) >= top_k:\n",
        "                break\n",
        "\n",
        "        return results, profile, weight_info\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Prompt builder (leaflet versija)\n",
        "# ============================================================\n",
        "def simple_llm_prompt_builder(query: str, chunks: List[Dict]) -> str:\n",
        "    context_blocks = []\n",
        "    for i, c in enumerate(chunks, start=1):\n",
        "        meta = (\n",
        "            f\"title={c.get('title', '')}, \"\n",
        "            f\"date={c.get('date', '')}, \"\n",
        "            f\"file={c.get('file_name', '')}, \"\n",
        "            f\"chunk_id={c.get('chunk_id', '')}\"\n",
        "        )\n",
        "        block = f\"[{i}] ({meta})\\n{c.get('text', '')}\"\n",
        "        context_blocks.append(block)\n",
        "\n",
        "    context_str = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Tu esi vēsturnieks, kurš analizē Latvijas komunistisko pagrīdes organizāciju skrejlapas (1934–1940).\n",
        "Tev ir pieejami tikai zemāk dotie skrejlapu fragmenti.\n",
        "\n",
        "Tavi metodoloģiskie principi:\n",
        "\n",
        "1. Atbildi uz jautājumu, balstoties TIKAI uz dotajiem fragmentiem. Nekādu ārēju zināšanu.\n",
        "2. NEIZDOMĀT faktus. Ja avotos nav tiešas norādes, tas jāklasificē kā nezināms.\n",
        "\n",
        "I. **Droši fakti (tieši avotos)**\n",
        "– Iekļauj tikai informāciju, kas skaidri minēta tekstā.\n",
        "– Katram faktam pievieno atsauci uz fragmentu (piem., “[3]”).\n",
        "– Ja iespējams, pievieno īsu citātu no avota.\n",
        "\n",
        "II. **Piesardzīgie secinājumi (netieši, bet atļauti)**\n",
        "– Atļauts tikai tad, ja secinājums loģiski izriet no fragmentu formulējumiem.\n",
        "– Vienmēr norādi, ka tas ir NETIEŠS secinājums.\n",
        "\n",
        "III. **Nezināmais**\n",
        "– Skaidri norādi visu, ko no avotiem noteikt NAV iespējams.\n",
        "– Šo sadaļu vienmēr iekļauj.\n",
        "– Ja atbilde nav nosakāma, skaidri uzraksti:\n",
        "  **\"To nav iespējams noteikt, balstoties tikai uz šeit dotajiem avotiem.\"**\n",
        "\n",
        "4. Stilam jābūt akadēmiski precīzam, konspektīvam, bez retorikas un vispārinājumiem.\n",
        "5. Atsaucies tikai uz informāciju, kas patiešām ir fragmentos.\n",
        "\n",
        "---\n",
        "\n",
        "Skrejlapu fragmenti:\n",
        "{context_str}\n",
        "\n",
        "Jautājums:\n",
        "{query}\n",
        "\n",
        "Tagad sniedz īsu, stingri strukturētu atbildi LATVIEŠU valodā tieši šādā formā:\n",
        "\n",
        "**I. Droši fakti (tieši avotos)**\n",
        "– ...\n",
        "\n",
        "**II. Piesardzīgie secinājumi (no fragmentiem izrietoši)**\n",
        "– ...\n",
        "\n",
        "**III. Nezināmais**\n",
        "– ...\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GUI helpers (leaflet filters)\n",
        "# ============================================================\n",
        "def build_filters_from_inputs(\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        ") -> Dict:\n",
        "    df = parse_user_date_box(date_from_str, is_start=True) if date_from_str else None\n",
        "    dt = parse_user_date_box(date_to_str, is_start=False) if date_to_str else None\n",
        "\n",
        "    pr_min = None\n",
        "    if print_run_min is not None and str(print_run_min).strip() != \"\":\n",
        "        try:\n",
        "            v = int(print_run_min)\n",
        "            if v > 0:\n",
        "                pr_min = v\n",
        "        except ValueError:\n",
        "            pr_min = None\n",
        "\n",
        "    pr_max = None\n",
        "    if print_run_max is not None and str(print_run_max).strip() != \"\":\n",
        "        try:\n",
        "            v = int(print_run_max)\n",
        "            if v > 0:\n",
        "                pr_max = v\n",
        "        except ValueError:\n",
        "            pr_max = None\n",
        "\n",
        "    org_substrings: List[str] = []\n",
        "    custom = (org_custom or \"\").strip().lower()\n",
        "    if custom:\n",
        "        org_substrings.append(custom)\n",
        "\n",
        "    org_substrings = sorted({s for s in org_substrings if s})\n",
        "\n",
        "    return {\n",
        "        \"date_from\": df,\n",
        "        \"date_to\": dt,\n",
        "        \"print_run_min\": pr_min,\n",
        "        \"print_run_max\": pr_max,\n",
        "        \"include_unk_print_run\": bool(include_unk_print_run),\n",
        "        \"org_substrings\": org_substrings,\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Global state\n",
        "# ============================================================\n",
        "global_rag = None\n",
        "global_leaflets = None\n",
        "\n",
        "\n",
        "def build_rag_from_zip_gui(zip_file):\n",
        "    global global_rag, global_leaflets\n",
        "\n",
        "    if zip_file is None:\n",
        "        return \"Nav augšupielādēts ZIP fails.\", \"\"\n",
        "\n",
        "    zip_path = zip_file.name\n",
        "\n",
        "    try:\n",
        "        leaflets = load_leaflets_from_zip(zip_path)\n",
        "    except Exception as e:\n",
        "        return f\"Kļūda, lasot ZIP: {e}\", \"\"\n",
        "\n",
        "    if not leaflets:\n",
        "        return \"Neizdevās nolasīt nevienu skrejlapu no ZIP.\", \"\"\n",
        "\n",
        "    rag = LeafletRAGHybrid()\n",
        "    rag.build_index(leaflets, max_words_per_chunk=260, overlap_words=60)\n",
        "\n",
        "    global_rag = rag\n",
        "    global_leaflets = leaflets\n",
        "\n",
        "    info = (\n",
        "        f\"Indekss uzbūvēts. Ielādētas {len(leaflets)} skrejlapas. \"\n",
        "        f\"Kopējais fragmentu skaits: {len(rag.chunks)}.\"\n",
        "    )\n",
        "    return info, \"\"\n",
        "\n",
        "\n",
        "def qa_on_corpus_gui(\n",
        "    api_key: str,\n",
        "    question: str,\n",
        "    top_k: int,\n",
        "    preview_chars: int,\n",
        "    min_score: float,\n",
        "    model_choice: str,\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        "    show_full_chunks: bool,\n",
        "    temperature: float,\n",
        "    weight_mode: str,\n",
        "    sem_percent: float,\n",
        "):\n",
        "    global global_rag\n",
        "\n",
        "    if global_rag is None:\n",
        "        return (\n",
        "            \"Indekss vēl nav uzbūvēts. Lūdzu augšupielādē ZIP un nospied 'Izveidot indeksu'.\",\n",
        "            \"\",\n",
        "        )\n",
        "\n",
        "    if not api_key.strip():\n",
        "        return \"Nav norādīts OpenRouter API key. Lūdzu ievadi savu API key.\", \"\"\n",
        "\n",
        "    if not question.strip():\n",
        "        return \"Lūdzu ievadi jautājumu.\", \"\"\n",
        "\n",
        "    effective_model_id = (model_choice or DEFAULT_MODEL_ID).strip()\n",
        "\n",
        "    filters = build_filters_from_inputs(\n",
        "        date_from_str,\n",
        "        date_to_str,\n",
        "        print_run_min,\n",
        "        print_run_max,\n",
        "        org_custom,\n",
        "        include_unk_print_run,\n",
        "    )\n",
        "\n",
        "    # (A) retrieve (SEM+BM25) + leaflet filters\n",
        "    retrieved, profile, weight_info = global_rag.retrieve(\n",
        "        query=question,\n",
        "        top_k=top_k,\n",
        "        filters=filters,\n",
        "        weight_mode=weight_mode,\n",
        "        sem_percent=sem_percent,\n",
        "    )\n",
        "\n",
        "    # (B) min_score filtrs PIRMS LLM (kā tev BM25 programmā)\n",
        "    effective_min_score = float(min_score) if min_score is not None else 0.0\n",
        "    if effective_min_score < 0.0:\n",
        "        effective_min_score = 0.0\n",
        "\n",
        "    if effective_min_score > 0.0:\n",
        "        retrieved = [c for c in retrieved if c.get(\"score\", 0.0) >= effective_min_score]\n",
        "\n",
        "    # Ja nav fragmentu — LLM nesaucam; logā rakstām tukšu retrieved_chunks\n",
        "    if not retrieved:\n",
        "        answer_text = (\n",
        "            f\"Netika atrasts neviens fragments ar līdzības score \"\n",
        "            f\"≥ {effective_min_score:.2f}.\"\n",
        "        )\n",
        "        log_qa_event(\n",
        "            question=question,\n",
        "            answer=answer_text,\n",
        "            retrieved_chunks=[],\n",
        "            model_id=effective_model_id,\n",
        "            top_k=top_k,\n",
        "            temperature=temperature,\n",
        "            query_profile=profile,\n",
        "            weight_info=weight_info,\n",
        "        )\n",
        "        return answer_text, \"\"\n",
        "\n",
        "    # (C) LLM prompt (redz tikai šos retrieved)\n",
        "    prompt = simple_llm_prompt_builder(question, retrieved)\n",
        "    answer_text = ask(prompt, api_key, model_id=effective_model_id, temperature=temperature)\n",
        "\n",
        "    # (D) normalize answer logam\n",
        "    if answer_text is None:\n",
        "        answer_for_log = \"[NONE_ANSWER_FROM_MODEL]\"\n",
        "    else:\n",
        "        stripped = answer_text.strip()\n",
        "        answer_for_log = \"[EMPTY_OR_WHITESPACE_ANSWER]\" if stripped == \"\" else answer_text\n",
        "\n",
        "    # (E) LOG: tikai promptā ieliktie chunk (=retrieved)\n",
        "    log_qa_event(\n",
        "        question=question,\n",
        "        answer=answer_for_log,\n",
        "        retrieved_chunks=retrieved,\n",
        "        model_id=effective_model_id,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        query_profile=profile,\n",
        "        weight_info=weight_info,\n",
        "    )\n",
        "\n",
        "    # (F) GUI preview\n",
        "    preview_lines = []\n",
        "    preview_lines.append(\n",
        "        f\"[WEIGHTS] mode={weight_info['mode']} | sem={weight_info['sem_percent']:.0f}% | bm25={weight_info['bm25_percent']:.0f}%\"\n",
        "    )\n",
        "    preview_lines.append(\n",
        "        f\"[QUERY_TYPE] {profile.qtype} | alpha_auto={profile.alpha:.2f} | \"\n",
        "        f\"semCand={profile.sem_candidates} | bm25Cand={profile.bm25_candidates}\"\n",
        "    )\n",
        "\n",
        "    if effective_min_score > 0.0:\n",
        "        preview_lines.append(\n",
        "            f\"[INFO] Pēc min score {effective_min_score:.2f} filtrēšanas izmantoti \"\n",
        "            f\"{len(retrieved)} fragmenti (no top_k={top_k}).\"\n",
        "        )\n",
        "\n",
        "    for i, c in enumerate(retrieved, start=1):\n",
        "        text = c.get(\"text\", \"\") or \"\"\n",
        "        if not show_full_chunks and len(text) > preview_chars:\n",
        "            text = text[:preview_chars] + \"...\"\n",
        "\n",
        "        meta = (\n",
        "            f\"[{i}] score={c.get('score', 0):.4f} | \"\n",
        "            f\"sem={c.get('score_semantic', 0):.4f} | \"\n",
        "            f\"bm25={c.get('score_bm25', 0):.4f} | \"\n",
        "            f\"title={c.get('title','')} \"\n",
        "            f\"date={c.get('date','')} \"\n",
        "            f\"print_run={c.get('print_run','')} \"\n",
        "            f\"author={c.get('author','')} \"\n",
        "            f\"file={c.get('file_name','')} \"\n",
        "            f\"chunk_id={c.get('chunk_id','')}\"\n",
        "        )\n",
        "        preview_lines.append(meta + \"\\n\" + text)\n",
        "\n",
        "    preview_block = \"\\n\\n---\\n\\n\".join(preview_lines)\n",
        "    return answer_text, preview_block\n",
        "\n",
        "\n",
        "def retrieve_only_gui(\n",
        "    question: str,\n",
        "    top_k: int,\n",
        "    preview_chars: int,\n",
        "    min_score: float,\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        "    show_full_chunks: bool,\n",
        "    weight_mode: str,\n",
        "    sem_percent: float,\n",
        "):\n",
        "    global global_rag\n",
        "\n",
        "    if global_rag is None:\n",
        "        return \"Indekss vēl nav uzbūvēts. Lūdzu vispirms uzbūvē indeksu.\"\n",
        "\n",
        "    if not question.strip():\n",
        "        return \"Lūdzu ievadi jautājumu.\"\n",
        "\n",
        "    filters = build_filters_from_inputs(\n",
        "        date_from_str,\n",
        "        date_to_str,\n",
        "        print_run_min,\n",
        "        print_run_max,\n",
        "        org_custom,\n",
        "        include_unk_print_run,\n",
        "    )\n",
        "\n",
        "    retrieved, profile, weight_info = global_rag.retrieve(\n",
        "        query=question,\n",
        "        top_k=top_k,\n",
        "        filters=filters,\n",
        "        weight_mode=weight_mode,\n",
        "        sem_percent=sem_percent,\n",
        "    )\n",
        "\n",
        "    effective_min_score = float(min_score) if min_score is not None else 0.0\n",
        "    if effective_min_score < 0.0:\n",
        "        effective_min_score = 0.0\n",
        "    if effective_min_score > 0.0:\n",
        "        retrieved = [c for c in retrieved if c.get(\"score\", 0.0) >= effective_min_score]\n",
        "\n",
        "    if not retrieved:\n",
        "        if effective_min_score > 0.0:\n",
        "            return (\n",
        "                f\"Nav atrasts neviens fragments ar līdzības score \"\n",
        "                f\"≥ {effective_min_score:.2f} (no top_k={top_k}).\"\n",
        "            )\n",
        "        return \"Nav atrasts neviens atbilstošs fragments.\"\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        f\"[WEIGHTS] mode={weight_info['mode']} | sem={weight_info['sem_percent']:.0f}% | bm25={weight_info['bm25_percent']:.0f}%\"\n",
        "    )\n",
        "    lines.append(\n",
        "        f\"[QUERY_TYPE] {profile.qtype} | alpha_auto={profile.alpha:.2f} | \"\n",
        "        f\"semCand={profile.sem_candidates} | bm25Cand={profile.bm25_candidates}\"\n",
        "    )\n",
        "    if effective_min_score > 0.0:\n",
        "        lines.append(\n",
        "            f\"[INFO] Pēc min score {effective_min_score:.2f} filtrēšanas izmantoti \"\n",
        "            f\"{len(retrieved)} fragmenti (no top_k={top_k}).\"\n",
        "        )\n",
        "\n",
        "    for i, c in enumerate(retrieved, start=1):\n",
        "        text = c.get(\"text\", \"\") or \"\"\n",
        "        if not show_full_chunks and len(text) > preview_chars:\n",
        "            text = text[:preview_chars] + \"...\"\n",
        "\n",
        "        meta = (\n",
        "            f\"[{i}] score={c.get('score', 0):.4f} | \"\n",
        "            f\"sem={c.get('score_semantic', 0):.4f} | \"\n",
        "            f\"bm25={c.get('score_bm25', 0):.4f} | \"\n",
        "            f\"title={c.get('title','')} \"\n",
        "            f\"date={c.get('date','')} \"\n",
        "            f\"print_run={c.get('print_run','')} \"\n",
        "            f\"author={c.get('author','')} \"\n",
        "            f\"file={c.get('file_name','')} \"\n",
        "            f\"chunk_id={c.get('chunk_id','')}\"\n",
        "        )\n",
        "        lines.append(meta + \"\\n\" + text)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# GRADIO UI\n",
        "# ============================================================\n",
        "with gr.Blocks() as gui:\n",
        "    gr.Markdown(\n",
        "        \"## Latvijas komunistisko organizāciju skrejlapu RAG asistents (1934–1940)\\n\"\n",
        "        \"ZIP ar skrejlapu .txt (ar metadatiem un `text:`) + jautājumi.\\n\"\n",
        "        \"Retrieval: **SEM + BM25** (AUTO/MANUAL). Atbildes balstītas TIKAI uz fragmentiem.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            api_key_box = gr.Textbox(\n",
        "                label=\"OpenRouter API key\",\n",
        "                type=\"password\",\n",
        "                placeholder=\"ievadi savu OpenRouter API key šeit\",\n",
        "            )\n",
        "\n",
        "            model_choice_box = gr.Dropdown(\n",
        "                label=\"OpenRouter modelis (vari izvēlēties vai ierakstīt pats)\",\n",
        "                choices=[\n",
        "                    DEFAULT_MODEL_ID,\n",
        "                    \"anthropic/claude-3.5-sonnet\",\n",
        "                    \"anthropic/claude-3.5-haiku\",\n",
        "\n",
        "                    \"openai/gpt-4.1\",\n",
        "                    \"openai/gpt-4.1-mini\",\n",
        "                    \"openai/gpt-4o\",\n",
        "                    \"openai/gpt-4o-mini\",\n",
        "\n",
        "                    \"qwen/qwen-2.5-7b-instruct\",\n",
        "\n",
        "                    \"deepseek/deepseek-chat\",\n",
        "\n",
        "                    \"mistralai/mistral-large-2512\",\n",
        "                    \"mistralai/mistral-small-3.2-24b-instruct\",\n",
        "                    \"mistralai/mistral-nemo\",\n",
        "\n",
        "                    \"meta-llama/llama-3.1-70b-instruct\",\n",
        "                    \"meta-llama/llama-3.1-8b-instruct\",\n",
        "\n",
        "                    \"google/gemini-2.5-flash\",\n",
        "                    \"google/gemini-2.5-flash-lite\",\n",
        "                    \"google/gemini-2.5-pro\",\n",
        "\n",
        "                    \"amazon/nova-2-lite-v1:free\",\n",
        "                    \"mistralai/mistral-7b-instruct:free\",\n",
        "                    \"kwaipilot/kat-coder-pro:free\",\n",
        "                    \"tngtech/deepseek-r1t2-chimera:free\",\n",
        "                ],\n",
        "                value=DEFAULT_MODEL_ID,\n",
        "                allow_custom_value=True,\n",
        "            )\n",
        "\n",
        "            temperature_inp = gr.Slider(\n",
        "                label=\"Temperature (0.0 = mazāka variācija, 1.0 = lielāka variācija)\",\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.0,\n",
        "                step=0.05,\n",
        "            )\n",
        "\n",
        "            zip_input = gr.File(label=\"ZIP ar LKP skrejlapu .txt failiem\")\n",
        "            build_btn = gr.Button(\"Izveidot indeksu\")\n",
        "            build_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            top_k_inp = gr.Slider(\n",
        "                label=\"Cik fragmentus izmantot (top_k)?\",\n",
        "                minimum=1,\n",
        "                maximum=30,\n",
        "                value=8,\n",
        "                step=1,\n",
        "            )\n",
        "\n",
        "            # WEIGHTS (NEW)\n",
        "            weight_mode_box = gr.Radio(\n",
        "                label=\"Svara režīms (AUTO vai MANUAL)\",\n",
        "                choices=[AUTO_MODE, MANUAL_MODE],\n",
        "                value=AUTO_MODE,\n",
        "            )\n",
        "\n",
        "            sem_weight_slider = gr.Slider(\n",
        "                label=\"Semantika (%) [MANUAL] (BM25 = 100 - Semantika)\",\n",
        "                minimum=0,\n",
        "                maximum=100,\n",
        "                value=40,\n",
        "                step=1,\n",
        "            )\n",
        "\n",
        "            preview_chars_inp = gr.Slider(\n",
        "                label=\"Cik simbolus rādīt katrā fragmenta preview?\",\n",
        "                minimum=50,\n",
        "                maximum=1000,\n",
        "                value=300,\n",
        "                step=50,\n",
        "            )\n",
        "\n",
        "            min_score_inp = gr.Slider(\n",
        "                label=\"Minimālais līdzības score (0 = izslēgts)\",\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.0,\n",
        "                step=0.01,\n",
        "            )\n",
        "\n",
        "            show_full_chunks_box = gr.Checkbox(\n",
        "                label=\"Rādīt pilnus fragmentus (nevis tikai preview)\",\n",
        "                value=True,\n",
        "            )\n",
        "\n",
        "            # Leaflet filters (paliek)\n",
        "            date_from_box = gr.Textbox(\n",
        "                label=\"Datums no (YYYY, YYYY-MM vai YYYY-MM-DD, tukšs – nav filtra)\",\n",
        "                placeholder=\"piem., 1934-01\",\n",
        "            )\n",
        "            date_to_box = gr.Textbox(\n",
        "                label=\"Datums līdz (YYYY, YYYY-MM vai YYYY-MM-DD, tukšs – nav filtra)\",\n",
        "                placeholder=\"piem., 1936-12\",\n",
        "            )\n",
        "\n",
        "            print_run_min_box = gr.Number(\n",
        "                label=\"Tirāža no (>=, tukšs – nav filtra)\",\n",
        "                value=None,\n",
        "                precision=0,\n",
        "            )\n",
        "            print_run_max_box = gr.Number(\n",
        "                label=\"Tirāža līdz (<=, tukšs – nav filtra)\",\n",
        "                value=None,\n",
        "                precision=0,\n",
        "            )\n",
        "\n",
        "            include_unk_print_run_box = gr.Checkbox(\n",
        "                label=\"Iekļaut skrejlapas ar nezināmu tirāžu (unk), ja ir tirāžas filtrs\",\n",
        "                value=True,\n",
        "            )\n",
        "\n",
        "            org_custom_box = gr.Textbox(\n",
        "                label=\"Papildu organizācijas filtrs (brīvs teksts, pēc apakšvirknes)\",\n",
        "                placeholder=\"piem., LKP CK, Rīgas komiteja, Sarkanā palīdzība, VEF, Daugavpils\",\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            question_box = gr.Textbox(\n",
        "                label=\"Jautājums par Latvijas komunistisko organizāciju skrejlapu korpusu\",\n",
        "                lines=3,\n",
        "                placeholder=\"Piemēram: Nosauc, kuriem komunistiem piesprieda nāvessodu Ulmaņa režīma laikā!\",\n",
        "            )\n",
        "            ask_btn = gr.Button(\"Uzdot jautājumu\")\n",
        "            retrieve_btn = gr.Button(\"Rādīt tikai fragmentus (bez LLM)\")\n",
        "\n",
        "            answer_out = gr.Markdown(label=\"Atbilde\")\n",
        "            chunks_out = gr.Textbox(\n",
        "                label=\"Izmantotie fragmenti (preview vai pilni)\",\n",
        "                lines=20,\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                log_btn = gr.Button(\"Izveidot un lejupielādēt žurnālu (JSON)\")\n",
        "                log_file_out = gr.File(\n",
        "                    label=f\"Žurnāla fails ({LOG_PATH})\",\n",
        "                    interactive=False,\n",
        "                )\n",
        "\n",
        "    build_btn.click(\n",
        "        fn=build_rag_from_zip_gui,\n",
        "        inputs=[zip_input],\n",
        "        outputs=[build_status, chunks_out],\n",
        "    )\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=qa_on_corpus_gui,\n",
        "        inputs=[\n",
        "            api_key_box,\n",
        "            question_box,\n",
        "            top_k_inp,\n",
        "            preview_chars_inp,\n",
        "            min_score_inp,\n",
        "            model_choice_box,\n",
        "            date_from_box,\n",
        "            date_to_box,\n",
        "            print_run_min_box,\n",
        "            print_run_max_box,\n",
        "            org_custom_box,\n",
        "            include_unk_print_run_box,\n",
        "            show_full_chunks_box,\n",
        "            temperature_inp,\n",
        "            weight_mode_box,\n",
        "            sem_weight_slider,\n",
        "        ],\n",
        "        outputs=[answer_out, chunks_out],\n",
        "    )\n",
        "\n",
        "    retrieve_btn.click(\n",
        "        fn=retrieve_only_gui,\n",
        "        inputs=[\n",
        "            question_box,\n",
        "            top_k_inp,\n",
        "            preview_chars_inp,\n",
        "            min_score_inp,\n",
        "            date_from_box,\n",
        "            date_to_box,\n",
        "            print_run_min_box,\n",
        "            print_run_max_box,\n",
        "            org_custom_box,\n",
        "            include_unk_print_run_box,\n",
        "            show_full_chunks_box,\n",
        "            weight_mode_box,\n",
        "            sem_weight_slider,\n",
        "        ],\n",
        "        outputs=[chunks_out],\n",
        "    )\n",
        "\n",
        "    log_btn.click(\n",
        "        fn=get_log_file_for_gui,\n",
        "        inputs=[],\n",
        "        outputs=[log_file_out],\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gui.launch(inbrowser=True)"
      ]
    }
  ]
}