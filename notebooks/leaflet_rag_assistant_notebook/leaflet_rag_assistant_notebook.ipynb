{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio sentence-transformers faiss-cpu requests"
      ],
      "metadata": {
        "id": "cmpiJhBKDUQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92e5705-17b6-4bbf-efb4-ba2663ff2e27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "1LolAprIPTTh",
        "outputId": "69023174-fac2-4e16-c69e-f112cfc4a5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://db4e31ac353a7db1e1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db4e31ac353a7db1e1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tempfile\n",
        "from typing import List, Dict, Callable, Tuple, Optional\n",
        "from pathlib import Path\n",
        "from datetime import date, datetime\n",
        "import calendar\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests  # for OpenRouter HTTP calls\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 0) OpenRouter + Claude helper\n",
        "# ===========================\n",
        "\n",
        "DEFAULT_MODEL_ID = \"anthropic/claude-3.5-sonnet\"\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "LOG_PATH = \"lkp_rag_log.json\"\n",
        "\n",
        "\n",
        "def ask(\n",
        "    prompt: str,\n",
        "    api_key: str,\n",
        "    model_id: Optional[str] = None,\n",
        "    temperature: float = 0.2,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Call LLM via OpenRouter with a single user prompt.\n",
        "    model_id:\n",
        "      - ja None, izmanto DEFAULT_MODEL_ID\n",
        "      - citādi – lieto, ko nodeva GUI (dropdown vai custom)\n",
        "    temperature:\n",
        "      - no 0.0 līdz 1.0 (konkretizācijai / radošumam)\n",
        "    \"\"\"\n",
        "    if not api_key.strip():\n",
        "        raise RuntimeError(\"OpenRouter API key nav norādīts.\")\n",
        "\n",
        "    model_id = (model_id or DEFAULT_MODEL_ID).strip()\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key.strip()}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_id,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"Tu esi precīzs, kritisks vēsturnieks, kas analizē \"\n",
        "                    \"Latvijas komunistisko pagrīdes organizāciju skrejlapas (1934–1940).\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        \"temperature\": float(temperature),\n",
        "    }\n",
        "\n",
        "    resp = requests.post(OPENROUTER_URL, headers=headers, json=data, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 0b) Žurnāla (loga) palīgfunkcijas\n",
        "# ===========================\n",
        "\n",
        "def _serialize_filters_for_log(filters: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Padara filtrus JSON-serializējamus (datumi -> ISO string).\n",
        "    \"\"\"\n",
        "    out = dict(filters)\n",
        "    for key in (\"date_from\", \"date_to\"):\n",
        "        v = out.get(key)\n",
        "        if isinstance(v, date):\n",
        "            out[key] = v.isoformat()\n",
        "    return out\n",
        "\n",
        "\n",
        "def _serialize_chunk_for_log(chunk: Dict, preview_chars: int = 400) -> Dict:\n",
        "    \"\"\"\n",
        "    Izvelk tikai vajadzīgo informāciju no fragmenta + īsu teksta fragmentu.\n",
        "    \"\"\"\n",
        "    text = chunk.get(\"text\", \"\") or \"\"\n",
        "    if len(text) > preview_chars:\n",
        "        text_snippet = text[:preview_chars] + \"...\"\n",
        "    else:\n",
        "        text_snippet = text\n",
        "\n",
        "    return {\n",
        "        \"leaflet_id\": chunk.get(\"leaflet_id\"),\n",
        "        \"file_name\": chunk.get(\"file_name\"),\n",
        "        \"title\": chunk.get(\"title\"),\n",
        "        \"date\": chunk.get(\"date\"),\n",
        "        \"print_run\": chunk.get(\"print_run\"),\n",
        "        \"author\": chunk.get(\"author\"),\n",
        "        \"source\": chunk.get(\"source\"),\n",
        "        \"chunk_id\": chunk.get(\"chunk_id\"),\n",
        "        \"score\": chunk.get(\"score\"),\n",
        "        \"text_snippet\": text_snippet,\n",
        "    }\n",
        "\n",
        "\n",
        "def log_qa_event(\n",
        "    question: str,\n",
        "    answer: str,\n",
        "    retrieved_chunks: List[Dict],\n",
        "    filters: Dict,\n",
        "    model_id: str,\n",
        "    top_k: int,\n",
        "    temperature: float,\n",
        "    preview_chars_for_log: int = 400,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Pieraksta vienu Q&A sesiju JSON failā LOG_PATH kā elementu masīvā.\n",
        "    Struktūra failā: [ {event1}, {event2}, ... ]\n",
        "    \"\"\"\n",
        "    event = {\n",
        "        \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"model_id\": model_id,\n",
        "        \"top_k\": int(top_k),\n",
        "        \"temperature\": float(temperature),\n",
        "        \"filters\": _serialize_filters_for_log(filters or {}),\n",
        "        \"retrieved_chunks\": [\n",
        "            _serialize_chunk_for_log(c, preview_chars_for_log)\n",
        "            for c in (retrieved_chunks or [])\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(LOG_PATH):\n",
        "            try:\n",
        "                with open(LOG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read().strip()\n",
        "                    if content:\n",
        "                        data = json.loads(content)\n",
        "                    else:\n",
        "                        data = []\n",
        "            except Exception:\n",
        "                data = []\n",
        "        else:\n",
        "            data = []\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            data = [data]\n",
        "\n",
        "        data.append(event)\n",
        "\n",
        "        with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: could not write log: {e}\")\n",
        "\n",
        "\n",
        "def get_log_file_for_gui():\n",
        "    \"\"\"\n",
        "    Funkcija Gradio pogai:\n",
        "    - ja žurnāla fails neeksistē, izveido tukšu JSON masīvu [] ,\n",
        "    - atgriež ceļu uz LOG_PATH, lai to var lejupielādēt.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(LOG_PATH):\n",
        "        with open(LOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump([], f, ensure_ascii=False, indent=2)\n",
        "    return LOG_PATH\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 1) Palīgfunkcijas datumiem un tirāžai\n",
        "# ===========================\n",
        "\n",
        "def _parse_plain_date_to_range(s: str) -> Tuple[Optional[date], Optional[date]]:\n",
        "    \"\"\"\n",
        "    '1934-08-20' -> (1934-08-20, 1934-08-20)\n",
        "    '1934-08'    -> (1934-08-01, 1934-08-31)\n",
        "    '1934'       -> (1934-01-01, 1934-12-31)\n",
        "    Любой мусор -> (None, None)\n",
        "    \"\"\"\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return None, None\n",
        "\n",
        "    parts = s.split(\"-\")\n",
        "    try:\n",
        "        if len(parts) == 3:\n",
        "            y, m, d = map(int, parts)\n",
        "            dt = date(y, m, d)\n",
        "            return dt, dt\n",
        "        elif len(parts) == 2:\n",
        "            y, m = map(int, parts)\n",
        "            last_day = calendar.monthrange(y, m)[1]\n",
        "            return date(y, m, 1), date(y, m, last_day)\n",
        "        else:\n",
        "            y = int(parts[0])\n",
        "            return date(y, 1, 1), date(y, 12, 31)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def parse_metadata_date_to_range(date_str: str) -> Tuple[Optional[date], Optional[date]]:\n",
        "    \"\"\"\n",
        "    All variants:\n",
        "    - 'YYYY-MM-DD'\n",
        "    - 'YYYY-MM'\n",
        "    - '[YYYY-MM-DD...]'\n",
        "    - '[...YYYY-MM-DD]'\n",
        "    - '[YYYY-MM-DD..YYYY-MM-DD]'\n",
        "    \"\"\"\n",
        "    if not date_str:\n",
        "        return None, None\n",
        "\n",
        "    date_str = date_str.strip()\n",
        "\n",
        "    if not (date_str.startswith(\"[\") and date_str.endswith(\"]\")):\n",
        "        return _parse_plain_date_to_range(date_str)\n",
        "\n",
        "    inner = date_str[1:-1].strip()\n",
        "\n",
        "    if \"...\" in inner:\n",
        "        if inner.endswith(\"...\"):\n",
        "            left = inner[:-3].strip()\n",
        "            start, end = _parse_plain_date_to_range(left)\n",
        "            return start, None\n",
        "        elif inner.startswith(\"...\"):\n",
        "            right = inner[3:].strip()\n",
        "            start, end = _parse_plain_date_to_range(right)\n",
        "            return None, end\n",
        "\n",
        "    if \"..\" in inner:\n",
        "        left, right = inner.split(\"..\", 1)\n",
        "        left = left.strip()\n",
        "        right = right.strip()\n",
        "        s1, e1 = _parse_plain_date_to_range(left)\n",
        "        s2, e2 = _parse_plain_date_to_range(right)\n",
        "        start = s1\n",
        "        end = e2\n",
        "        return start, end\n",
        "\n",
        "    return _parse_plain_date_to_range(inner)\n",
        "\n",
        "\n",
        "def parse_print_run_value(v: str) -> Optional[int]:\n",
        "    \"\"\"\n",
        "    '1000' -> 1000\n",
        "    'unk'  -> None\n",
        "    'ap 5000' -> 5000\n",
        "    \"\"\"\n",
        "    if not v:\n",
        "        return None\n",
        "    v_low = v.strip().lower()\n",
        "    if v_low == \"unk\":\n",
        "        return None\n",
        "    digits = \"\".join(ch for ch in v_low if ch.isdigit())\n",
        "    if not digits:\n",
        "        return None\n",
        "    try:\n",
        "        return int(digits)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_user_date_box(s: str, is_start: bool) -> Optional[date]:\n",
        "    \"\"\"\n",
        "    GUI: 'Datums no' / 'Datums līdz' (YYYY, YYYY-MM, YYYY-MM-DD).\n",
        "    \"\"\"\n",
        "    s = (s or \"\").strip()\n",
        "    if not s:\n",
        "        return None\n",
        "    parts = s.split(\"-\")\n",
        "    if len(parts) == 3:\n",
        "        y, m, d = map(int, parts)\n",
        "        return date(y, m, d)\n",
        "    elif len(parts) == 2:\n",
        "        y, m = map(int, parts)\n",
        "        if is_start:\n",
        "            return date(y, m, 1)\n",
        "        else:\n",
        "            last_day = calendar.monthrange(y, m)[1]\n",
        "            return date(y, m, last_day)\n",
        "    else:\n",
        "        y = int(parts[0])\n",
        "        return date(y, 1, 1) if is_start else date(y, 12, 31)\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 1b) Loading and parsing leaflets from ZIP\n",
        "# ===========================\n",
        "\n",
        "def parse_metadata(content: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Parse metadata section from file content.\n",
        "    \"\"\"\n",
        "    parts = content.split(\"text:\", 1)\n",
        "    metadata_text = parts[0]\n",
        "\n",
        "    metadata: Dict = {\n",
        "        \"id\": None,\n",
        "        \"file_name\": \"\",\n",
        "        \"title\": \"\",\n",
        "        \"author\": \"\",\n",
        "        \"date\": \"\",\n",
        "        \"print_run\": \"\",\n",
        "        \"typography_name\": \"\",\n",
        "        \"source\": \"\",\n",
        "        \"text\": \"\",\n",
        "    }\n",
        "\n",
        "    for line in metadata_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if \":\" in line:\n",
        "            key, value = line.split(\":\", 1)\n",
        "            key = key.strip()\n",
        "            value = value.strip()\n",
        "\n",
        "            if key == \"id\":\n",
        "                try:\n",
        "                    metadata[key] = int(value)\n",
        "                except ValueError:\n",
        "                    metadata[key] = None\n",
        "            elif key in metadata:\n",
        "                metadata[key] = value\n",
        "\n",
        "    if len(parts) > 1:\n",
        "        metadata[\"text\"] = parts[1].strip()\n",
        "\n",
        "    raw_date = metadata.get(\"date\", \"\")\n",
        "    try:\n",
        "        d_start, d_end = parse_metadata_date_to_range(raw_date)\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"Warning: cannot parse date '{raw_date}' \"\n",
        "            f\"in file {metadata.get('file_name','')}: {e}\"\n",
        "        )\n",
        "        d_start, d_end = None, None\n",
        "\n",
        "    metadata[\"date_start\"] = d_start\n",
        "    metadata[\"date_end\"] = d_end\n",
        "    metadata[\"print_run_value\"] = parse_print_run_value(metadata.get(\"print_run\", \"\"))\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def load_leaflets_from_zip(zip_path: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load and parse leaflets from ZIP archive.\n",
        "    \"\"\"\n",
        "    results: List[Dict] = []\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(temp_dir)\n",
        "\n",
        "        corpus_dir: Optional[str] = None\n",
        "        if any(Path(temp_dir).glob(\"*.txt\")):\n",
        "            corpus_dir = temp_dir\n",
        "        else:\n",
        "            for item in os.listdir(temp_dir):\n",
        "                potential_corpus_dir = os.path.join(temp_dir, item)\n",
        "                if os.path.isdir(potential_corpus_dir) and any(\n",
        "                    Path(potential_corpus_dir).glob(\"*.txt\")\n",
        "                ):\n",
        "                    corpus_dir = potential_corpus_dir\n",
        "                    break\n",
        "\n",
        "        if not corpus_dir:\n",
        "            raise ValueError(\"Cannot find corpus directory with .txt files in ZIP file\")\n",
        "\n",
        "        for file_path in Path(corpus_dir).glob(\"*.txt\"):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                leaflet_data = parse_metadata(content)\n",
        "                leaflet_data[\"path\"] = str(file_path)\n",
        "                if not leaflet_data.get(\"file_name\"):\n",
        "                    leaflet_data[\"file_name\"] = file_path.name\n",
        "                if not leaflet_data.get(\"title\"):\n",
        "                    leaflet_data[\"title\"] = file_path.stem\n",
        "\n",
        "                results.append(leaflet_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 2) RAG utilities: chunking + class LeafletRAG\n",
        "# ===========================\n",
        "\n",
        "def chunk_text(text: str, max_words: int = 300) -> List[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "\n",
        "    chunks: List[str] = []\n",
        "    current: List[str] = []\n",
        "\n",
        "    for w in words:\n",
        "        current.append(w)\n",
        "        if len(current) >= max_words:\n",
        "            chunks.append(\" \".join(current))\n",
        "            current = []\n",
        "\n",
        "    if current:\n",
        "        chunks.append(\" \".join(current))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_matches_filters(chunk: Dict, filters: Dict) -> bool:\n",
        "    \"\"\"\n",
        "    Pārbauda, vai fragments atbilst GUI filtriem.\n",
        "    \"\"\"\n",
        "    # Datums\n",
        "    df = filters.get(\"date_from\")\n",
        "    dt = filters.get(\"date_to\")\n",
        "    if df or dt:\n",
        "        s = chunk.get(\"date_start\")\n",
        "        e = chunk.get(\"date_end\")\n",
        "        if s is None and e is None:\n",
        "            return False\n",
        "        if df and e is not None and e < df:\n",
        "            return False\n",
        "        if dt and s is not None and s > dt:\n",
        "            return False\n",
        "\n",
        "    # Tirāža\n",
        "    pr_min = filters.get(\"print_run_min\")\n",
        "    pr_max = filters.get(\"print_run_max\")\n",
        "    include_unk = bool(filters.get(\"include_unk_print_run\", False))\n",
        "\n",
        "    if pr_min is not None or pr_max is not None:\n",
        "        pr = chunk.get(\"print_run_value\")\n",
        "        if pr is None:\n",
        "            if not include_unk:\n",
        "                return False\n",
        "        else:\n",
        "            if pr_min is not None and pr < pr_min:\n",
        "                return False\n",
        "            if pr_max is not None and pr > pr_max:\n",
        "                return False\n",
        "\n",
        "    # Organizācija – substring search author+source+title+file_name\n",
        "    org_subs = filters.get(\"org_substrings\") or []\n",
        "    if org_subs:\n",
        "        org_meta = (\n",
        "            (chunk.get(\"author\", \"\") + \" \" +\n",
        "             chunk.get(\"source\", \"\") + \" \" +\n",
        "             chunk.get(\"title\", \"\") + \" \" +\n",
        "             chunk.get(\"file_name\", \"\"))\n",
        "            .lower()\n",
        "        )\n",
        "        if not any(sub in org_meta for sub in org_subs):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "class LeafletRAG:\n",
        "    \"\"\"\n",
        "    Simple RAG system for the Latvian Communist Leaflet Corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    ):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index: Optional[faiss.IndexFlatIP] = None\n",
        "        self.chunks: List[Dict] = []\n",
        "        self.embedding_dim: Optional[int] = None\n",
        "\n",
        "    def build_index(\n",
        "        self,\n",
        "        leaflets: List[Dict],\n",
        "        max_words_per_chunk: int = 260,\n",
        "        normalize_embeddings: bool = True,\n",
        "    ) -> None:\n",
        "        all_texts: List[str] = []\n",
        "        self.chunks = []\n",
        "\n",
        "        for leaflet in leaflets:\n",
        "            full_text = leaflet.get(\"text\", \"\")\n",
        "            if not full_text.strip():\n",
        "                continue\n",
        "\n",
        "            leaflet_id = leaflet.get(\"id\")\n",
        "            file_name = leaflet.get(\"file_name\", \"\")\n",
        "            title = leaflet.get(\"title\", \"\")\n",
        "            date_str = leaflet.get(\"date\", \"\")\n",
        "            date_start = leaflet.get(\"date_start\")\n",
        "            date_end = leaflet.get(\"date_end\")\n",
        "            print_run = leaflet.get(\"print_run\", \"\")\n",
        "            print_run_value = leaflet.get(\"print_run_value\")\n",
        "            author = leaflet.get(\"author\", \"\")\n",
        "            source = leaflet.get(\"source\", \"\")\n",
        "\n",
        "            chunk_list = chunk_text(full_text, max_words=max_words_per_chunk)\n",
        "            for i, chunk in enumerate(chunk_list):\n",
        "                self.chunks.append(\n",
        "                    {\n",
        "                        \"leaflet_id\": leaflet_id,\n",
        "                        \"file_name\": file_name,\n",
        "                        \"title\": title,\n",
        "                        \"date\": date_str,\n",
        "                        \"date_start\": date_start,\n",
        "                        \"date_end\": date_end,\n",
        "                        \"print_run\": print_run,\n",
        "                        \"print_run_value\": print_run_value,\n",
        "                        \"author\": author,\n",
        "                        \"source\": source,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"text\": chunk,\n",
        "                    }\n",
        "                )\n",
        "                all_texts.append(chunk)\n",
        "\n",
        "        if not all_texts:\n",
        "            raise ValueError(\"No text chunks found. Cannot build index.\")\n",
        "\n",
        "        embeddings = self.model.encode(all_texts, convert_to_numpy=True)\n",
        "\n",
        "        if normalize_embeddings:\n",
        "            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12\n",
        "            embeddings = embeddings / norms\n",
        "\n",
        "        self.embedding_dim = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def retrieve(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 5,\n",
        "        normalize_embeddings: bool = True,\n",
        "        filters: Optional[Dict] = None,\n",
        "    ) -> List[Dict]:\n",
        "        if self.index is None or not self.chunks:\n",
        "            raise RuntimeError(\"Index is not built. Call build_index() first.\")\n",
        "\n",
        "        if filters is None:\n",
        "            filters = {}\n",
        "\n",
        "        query_emb = self.model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "        if normalize_embeddings:\n",
        "            norms = np.linalg.norm(query_emb, axis=1, keepdims=True) + 1e-12\n",
        "            query_emb = query_emb / norms\n",
        "\n",
        "        search_k = min(max(top_k * 5, top_k + 20), len(self.chunks))\n",
        "        scores, indices = self.index.search(query_emb, search_k)\n",
        "        scores = scores[0]\n",
        "        indices = indices[0]\n",
        "\n",
        "        results: List[Dict] = []\n",
        "        for score, idx in zip(scores, indices):\n",
        "            if idx < 0 or idx >= len(self.chunks):\n",
        "                continue\n",
        "            chunk_info = self.chunks[idx].copy()\n",
        "            chunk_info[\"score\"] = float(score)\n",
        "\n",
        "            if not chunk_matches_filters(chunk_info, filters):\n",
        "                continue\n",
        "\n",
        "            results.append(chunk_info)\n",
        "            if len(results) >= top_k:\n",
        "                break\n",
        "\n",
        "        return results\n",
        "\n",
        "    def answer(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 5,\n",
        "        generator_fn: Optional[Callable[[str, List[Dict]], str]] = None,\n",
        "        filters: Optional[Dict] = None,\n",
        "    ) -> Tuple[str, List[Dict]]:\n",
        "        retrieved = self.retrieve(query, top_k=top_k, filters=filters)\n",
        "\n",
        "        if generator_fn is None:\n",
        "            context_text = \"\\n\\n---\\n\\n\".join(\n",
        "                f\"[{i+1}] {c['text']}\" for i, c in enumerate(retrieved)\n",
        "            )\n",
        "            answer_text = (\n",
        "                \"Simple concatenation answer.\\n\\n\"\n",
        "                f\"Query: {query}\\n\\n\"\n",
        "                \"Relevant leaflet chunks:\\n\\n\"\n",
        "                f\"{context_text}\"\n",
        "            )\n",
        "            return answer_text, retrieved\n",
        "\n",
        "        answer_text = generator_fn(query, retrieved)\n",
        "        return answer_text, retrieved\n",
        "\n",
        "\n",
        "def simple_llm_prompt_builder(query: str, chunks: List[Dict]) -> str:\n",
        "    context_blocks = []\n",
        "    for i, c in enumerate(chunks, start=1):\n",
        "        meta = (\n",
        "            f\"title={c.get('title', '')}, \"\n",
        "            f\"date={c.get('date', '')}, \"\n",
        "            f\"file={c.get('file_name', '')}, \"\n",
        "            f\"chunk_id={c.get('chunk_id', '')}\"\n",
        "        )\n",
        "        block = f\"[{i}] ({meta})\\n{c.get('text', '')}\"\n",
        "        context_blocks.append(block)\n",
        "\n",
        "    context_str = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Tu esi vēsturnieks, kurš analizē Latvijas komunistisko pagrīdes organizāciju skrejlapas (1934–1940).\n",
        "Tev ir pieejami tikai zemāk dotie skrejlapu fragmenti.\n",
        "\n",
        "Tavi metodoloģiskie principi:\n",
        "\n",
        "1. Atbildi uz jautājumu, balstoties TIKAI uz dotajiem fragmentiem. Nekādu ārēju zināšanu.\n",
        "2. NEIZDOMĀT faktus. Ja avotos nav tiešas norādes, tas jāklasificē kā nezināms.\n",
        "3. Katrai atbildes sadaļai ir stingras prasības:\n",
        "\n",
        "I. **Droši fakti (tieši avotos)**\n",
        "– Iekļauj tikai informāciju, kas skaidri minēta tekstā.\n",
        "– Katram faktam pievieno atsauci uz fragmentu (piem., “[3]”).\n",
        "– Ja iespējams, pievieno īsu citātu no avota (piem., “avots saka: ‘piesprieda nāves sodu Murinam’)\n",
        "– Nekad neapvieno divus atsevišķos avotos minētus faktus vienā, ja avots tos nesaista.\n",
        "\n",
        "II. **Piesardzīgie secinājumi (netieši, bet atļauti)**\n",
        "– Atļauts tikai tad, ja secinājums loģiski izriet no fragmentu formulējumiem.\n",
        "– Vienmēr norādi, ka tas ir NETIEŠS secinājums.\n",
        "– Nekad nepaplašini faktus ārpus tā, ko teksts atļauj.\n",
        "\n",
        "III. **Nezināmais**\n",
        "– Skaidri norādi visu, ko no avotiem noteikt NAV iespējams.\n",
        "– Šo sadaļu vienmēr iekļauj, pat ja šķiet, ka viss ir skaidrs.\n",
        "– Ja atbilde nav nosakāma, skaidri uzraksti:\n",
        "  **\"To nav iespējams noteikt, balstoties tikai на šeit dotajiem avotiem.\"**\n",
        "\n",
        "4. Stilam jābūt akadēmiski precīzam, konspektīvam, bez retorikas un vispārinājumiem.\n",
        "5. Atsaucies tikai uz informāciju, kas patiešām ir fragmentos.\n",
        "6. Nekad neizdari pieņēmumus par kontekstu, motīviem vai faktiem, kas nav tieši norādīti tekstā.\n",
        "\n",
        "---\n",
        "\n",
        "Skrejlapu fragmenti:\n",
        "{context_str}\n",
        "\n",
        "Jautājums:\n",
        "{query}\n",
        "\n",
        "Tagad sniedz īsu, stingri strukturētu atbildi LATVIEŠU valodā tieši šādā formā:\n",
        "\n",
        "**I. Droši fakti (tieši avotos)**\n",
        "– ...\n",
        "\n",
        "**II. Piesardzīgie secinājumi (no fragmentiem izrietoši)**\n",
        "– ...\n",
        "\n",
        "**III. Nezināmais**\n",
        "– ...\n",
        "\"\"\"\n",
        "    return prompt.strip()\n",
        "\n",
        "\n",
        "def print_retrieved_chunks(chunks: List[Dict], max_chars: int = 300) -> None:\n",
        "    print(\"\\n=== RETRIEVED CHUNKS ===\\n\")\n",
        "    for i, c in enumerate(chunks, start=1):\n",
        "        print(f\"[{i}] score={c.get('score', 0):.4f}\")\n",
        "        print(f\"    title     : {c.get('title', '')}\")\n",
        "        print(f\"    date      : {c.get('date', '')}\")\n",
        "        print(f\"    file_name : {c.get('file_name', '')}\")\n",
        "        print(f\"    chunk_id  : {c.get('chunk_id', '')}\")\n",
        "        print(f\"    print_run : {c.get('print_run', '')}\")\n",
        "        print(f\"    author    : {c.get('author', '')}\")\n",
        "        text = c.get(\"text\", \"\")\n",
        "        if len(text) > max_chars:\n",
        "            text = text[:max_chars] + \"...\"\n",
        "        print(\"    text:\")\n",
        "        print(\"    \" + text.replace(\"\\n\", \"\\n    \"))\n",
        "        print()\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 4) Gradio GUI – Latvijas komunistisko skrejlapu asistents\n",
        "# ===========================\n",
        "\n",
        "global_rag = None\n",
        "global_leaflets = None\n",
        "\n",
        "\n",
        "def build_rag_from_zip_gui(zip_file):\n",
        "    \"\"\"Augšupielādē Latvijas komunistisko skrejlapu ZIP un uzbūvē RAG indeksu.\"\"\"\n",
        "    global global_rag, global_leaflets\n",
        "\n",
        "    if zip_file is None:\n",
        "        return \"Nav augšupielādēts ZIP fails.\", \"\"\n",
        "\n",
        "    zip_path = zip_file.name\n",
        "\n",
        "    try:\n",
        "        leaflets = load_leaflets_from_zip(zip_path)\n",
        "    except Exception as e:\n",
        "        return f\"Kļūda, lasot ZIP: {e}\", \"\"\n",
        "\n",
        "    if not leaflets:\n",
        "        return \"Neizdevās nolasīt nevienu skrejlapu от ZIP.\", \"\"\n",
        "\n",
        "    rag = LeafletRAG()\n",
        "    rag.build_index(leaflets)\n",
        "\n",
        "    global_rag = rag\n",
        "    global_leaflets = leaflets\n",
        "\n",
        "    info = (\n",
        "        f\"Indekss uzbūvēts. Ielādētas {len(leaflets)} skrejlapas. \"\n",
        "        f\"Kopējais fragmentu skaits: {len(rag.chunks)}.\"\n",
        "    )\n",
        "    return info, \"\"\n",
        "\n",
        "\n",
        "def build_filters_from_inputs(\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        ") -> Dict:\n",
        "    df = parse_user_date_box(date_from_str, is_start=True) if date_from_str else None\n",
        "    dt = parse_user_date_box(date_to_str, is_start=False) if date_to_str else None\n",
        "\n",
        "    pr_min = None\n",
        "    if print_run_min is not None and str(print_run_min).strip() != \"\":\n",
        "        try:\n",
        "            v = int(print_run_min)\n",
        "            if v > 0:\n",
        "                pr_min = v\n",
        "        except ValueError:\n",
        "            pr_min = None\n",
        "\n",
        "    pr_max = None\n",
        "    if print_run_max is not None and str(print_run_max).strip() != \"\":\n",
        "        try:\n",
        "            v = int(print_run_max)\n",
        "            if v > 0:\n",
        "                pr_max = v\n",
        "        except ValueError:\n",
        "            pr_max = None\n",
        "\n",
        "    org_substrings: List[str] = []\n",
        "\n",
        "    custom = (org_custom or \"\").strip().lower()\n",
        "    if custom:\n",
        "        org_substrings.append(custom)\n",
        "\n",
        "    org_substrings = sorted({s for s in org_substrings if s})\n",
        "\n",
        "    return {\n",
        "        \"date_from\": df,\n",
        "        \"date_to\": dt,\n",
        "        \"print_run_min\": pr_min,\n",
        "        \"print_run_max\": pr_max,\n",
        "        \"include_unk_print_run\": bool(include_unk_print_run),\n",
        "        \"org_substrings\": org_substrings,\n",
        "    }\n",
        "\n",
        "\n",
        "def qa_on_corpus_gui(\n",
        "    api_key: str,\n",
        "    question: str,\n",
        "    top_k: int,\n",
        "    preview_chars: int,\n",
        "    min_score: float,\n",
        "    model_choice: str,\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        "    show_full_chunks: bool,\n",
        "    temperature: float,\n",
        "):\n",
        "    \"\"\"Atbild uz jautājumu par Latvijas komunistisko organizāciju skrejlapu korpusu.\"\"\"\n",
        "    global global_rag\n",
        "\n",
        "    if global_rag is None:\n",
        "        return (\n",
        "            \"Indekss vēl nav uzbūvēts. Lūdzu vispirms augšupielādē ZIP и nospied 'Izveidot indeksu'.\",\n",
        "            \"\",\n",
        "        )\n",
        "\n",
        "    if not api_key.strip():\n",
        "        return \"Nav norādīts OpenRouter API key. Lūdzu ievadi savu API key kreisajā pusē.\", \"\"\n",
        "\n",
        "    if not question.strip():\n",
        "        return \"Lūdzu ievadi jautājumu.\", \"\"\n",
        "\n",
        "    effective_model_id = (model_choice or DEFAULT_MODEL_ID).strip()\n",
        "\n",
        "    filters = build_filters_from_inputs(\n",
        "        date_from_str,\n",
        "        date_to_str,\n",
        "        print_run_min,\n",
        "        print_run_max,\n",
        "        org_custom,\n",
        "        include_unk_print_run,\n",
        "    )\n",
        "\n",
        "    # 0.0 -> filtrs izslēgts; > 0.0 -> filtrs ieslēgts\n",
        "    effective_min_score = float(min_score) if min_score is not None else 0.0\n",
        "    if effective_min_score < 0.0:\n",
        "        effective_min_score = 0.0\n",
        "\n",
        "    filters[\"min_score\"] = effective_min_score\n",
        "\n",
        "    def local_llm_generator(q: str, chunks: List[Dict]) -> str:\n",
        "        prompt = simple_llm_prompt_builder(q, chunks)\n",
        "        return ask(\n",
        "            prompt,\n",
        "            api_key,\n",
        "            model_id=effective_model_id,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    # 1) RAG + LLM (retrieved jau ar datuma/tirāžas/organizācijas filtru)\n",
        "    answer_text, retrieved = global_rag.answer(\n",
        "        question,\n",
        "        top_k=top_k,\n",
        "        generator_fn=local_llm_generator,\n",
        "        filters=filters,\n",
        "    )\n",
        "\n",
        "    # 2) Piemēro min_score только preview/logam\n",
        "    if effective_min_score > 0.0:\n",
        "        retrieved = [\n",
        "            c for c in retrieved\n",
        "            if c.get(\"score\", 0.0) >= effective_min_score\n",
        "        ]\n",
        "\n",
        "    # 3) Logging – žurnālā liekam jau pēc score-filtra\n",
        "    if answer_text is None:\n",
        "        answer_for_log = \"[NONE_ANSWER_FROM_MODEL]\"\n",
        "    else:\n",
        "        stripped = answer_text.strip()\n",
        "        answer_for_log = stripped if stripped else \"[EMPTY_OR_WHITESPACE_ANSWER]\"\n",
        "\n",
        "    log_qa_event(\n",
        "        question=question,\n",
        "        answer=answer_for_log,\n",
        "        retrieved_chunks=retrieved,\n",
        "        filters=filters,\n",
        "        model_id=effective_model_id,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        preview_chars_for_log=preview_chars,\n",
        "    )\n",
        "\n",
        "    # 4) Ja pēc score-filtra nav fragmentu\n",
        "    if not retrieved:\n",
        "        if effective_min_score > 0.0:\n",
        "            info_msg = (\n",
        "                f\"Nav atrasts neviens fragments ar līdzības score \"\n",
        "                f\"≥ {effective_min_score:.2f} (no top_k={top_k}).\"\n",
        "            )\n",
        "        else:\n",
        "            info_msg = \"Nav atrasts neviens atbilstošs fragments.\"\n",
        "        return answer_text, info_msg\n",
        "\n",
        "    # 5) Preview priekš GUI\n",
        "    preview_lines = []\n",
        "\n",
        "    if effective_min_score > 0.0:\n",
        "        used_count = len(retrieved)\n",
        "        preview_lines.append(\n",
        "            f\"[INFO] Pēc min score {effective_min_score:.2f} filtrēšanas izmantoti \"\n",
        "            f\"{used_count} fragmenti (no top_k={top_k}).\"\n",
        "        )\n",
        "\n",
        "    for i, c in enumerate(retrieved, start=1):\n",
        "        text = c.get(\"text\", \"\")\n",
        "        if not show_full_chunks and len(text) > preview_chars:\n",
        "            text = text[:preview_chars] + \"...\"\n",
        "\n",
        "        meta = (\n",
        "            f\"[{i}] score={c.get('score', 0):.4f} | \"\n",
        "            f\"title={c.get('title','')}\"\n",
        "            f\" date={c.get('date','')}\"\n",
        "            f\" print_run={c.get('print_run','')}\"\n",
        "            f\" author={c.get('author','')}\"\n",
        "            f\" file={c.get('file_name','')}\"\n",
        "            f\" chunk_id={c.get('chunk_id','')}\"\n",
        "        )\n",
        "        preview_lines.append(meta + \"\\n\" + text)\n",
        "\n",
        "    preview_block = \"\\n\\n---\\n\\n\".join(preview_lines)\n",
        "    return answer_text, preview_block\n",
        "\n",
        "\n",
        "def retrieve_only_gui(\n",
        "    question: str,\n",
        "    top_k: int,\n",
        "    preview_chars: int,\n",
        "    min_score: float,\n",
        "    date_from_str: str,\n",
        "    date_to_str: str,\n",
        "    print_run_min,\n",
        "    print_run_max,\n",
        "    org_custom: str,\n",
        "    include_unk_print_run: bool,\n",
        "    show_full_chunks: bool,\n",
        "):\n",
        "    \"\"\"\n",
        "    Tikai retrīvs: parāda fragmentus (bez LLM, bez OpenRouter, bez API key).\n",
        "    \"\"\"\n",
        "    global global_rag\n",
        "\n",
        "    if global_rag is None:\n",
        "        return \"Indekss vēl nav uzbūvēts. Lūdzu vispirms uzbūvē indeksu.\"\n",
        "\n",
        "    if not question.strip():\n",
        "        return \"Lūdzu ievadi jautājumu.\"\n",
        "\n",
        "    filters = build_filters_from_inputs(\n",
        "        date_from_str,\n",
        "        date_to_str,\n",
        "        print_run_min,\n",
        "        print_run_max,\n",
        "        org_custom,\n",
        "        include_unk_print_run,\n",
        "    )\n",
        "\n",
        "    effective_min_score = float(min_score) if min_score is not None else 0.0\n",
        "    if effective_min_score < 0.0:\n",
        "        effective_min_score = 0.0\n",
        "    filters[\"min_score\"] = effective_min_score\n",
        "\n",
        "    retrieved = global_rag.retrieve(\n",
        "        query=question,\n",
        "        top_k=top_k,\n",
        "        filters=filters,\n",
        "    )\n",
        "\n",
        "    if effective_min_score > 0.0:\n",
        "        retrieved = [\n",
        "            c for c in retrieved\n",
        "            if c.get(\"score\", 0.0) >= effective_min_score\n",
        "        ]\n",
        "\n",
        "    if not retrieved:\n",
        "        if effective_min_score > 0.0:\n",
        "            return (\n",
        "                f\"Nav atrasts neviens fragments ar līdzības score \"\n",
        "                f\"≥ {effective_min_score:.2f} (no top_k={top_k}).\"\n",
        "            )\n",
        "        else:\n",
        "            return \"Nav atrasts neviens atbilstošs fragments.\"\n",
        "\n",
        "    lines = []\n",
        "\n",
        "    if effective_min_score > 0.0:\n",
        "        used_count = len(retrieved)\n",
        "        lines.append(\n",
        "            f\"[INFO] Pēc min score {effective_min_score:.2f} filtrēšanas izmantoti \"\n",
        "            f\"{used_count} fragmenti (no top_k={top_k}).\"\n",
        "        )\n",
        "\n",
        "    for i, c in enumerate(retrieved, start=1):\n",
        "        text = c.get(\"text\", \"\")\n",
        "        if not show_full_chunks and len(text) > preview_chars:\n",
        "            text = text[:preview_chars] + \"...\"\n",
        "\n",
        "        meta = (\n",
        "            f\"[{i}] score={c.get('score', 0):.4f} | \"\n",
        "            f\"title={c.get('title','')}\"\n",
        "            f\" date={c.get('date','')}\"\n",
        "            f\" print_run={c.get('print_run','')}\"\n",
        "            f\" author={c.get('author','')}\"\n",
        "            f\" file={c.get('file_name','')}\"\n",
        "            f\" chunk_id={c.get('chunk_id','')}\"\n",
        "        )\n",
        "\n",
        "        lines.append(meta + \"\\n\" + text)\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "with gr.Blocks() as gui:\n",
        "    gr.Markdown(\n",
        "        \"## Latvijas komunistisko organizāciju skrejlapu RAG asistents (1934–1940)\\n\"\n",
        "        \"Augšupielādē Latvijas komunistisko organizāciju skrejlapu korpusa ZIP failu un uzdod vēsturiskus jautājumus.\\n\"\n",
        "        \"Atbildes balstītas TIKAI uz skrejlapu tekstiem.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            api_key_box = gr.Textbox(\n",
        "                label=\"OpenRouter API key\",\n",
        "                type=\"password\",\n",
        "                placeholder=\"ievadi savu OpenRouter API key šeit\",\n",
        "            )\n",
        "\n",
        "            model_choice_box = gr.Dropdown(\n",
        "                label=\"OpenRouter modelis (vari izvēlēties vai ierakstīt pats)\",\n",
        "                choices=[\n",
        "                    DEFAULT_MODEL_ID,\n",
        "                    \"anthropic/claude-3.5-haiku\",\n",
        "\n",
        "                    \"openai/gpt-4.1\",\n",
        "                    \"openai/gpt-4.1-mini\",\n",
        "                    \"openai/gpt-4o\",\n",
        "                    \"openai/gpt-4o-mini\",\n",
        "\n",
        "                    \"qwen/qwen-2.5-7b-instruct\",\n",
        "\n",
        "                    \"deepseek/deepseek-chat\",\n",
        "\n",
        "                    \"mistralai/mistral-large-2512\",\n",
        "                    \"mistralai/mistral-small-3.2-24b-instruct\",\n",
        "                    \"mistralai/mistral-nemo\",\n",
        "\n",
        "                    \"meta-llama/llama-3.1-70b-instruct\",\n",
        "                    \"meta-llama/llama-3.1-8b-instruct\",\n",
        "\n",
        "                    \"google/gemini-2.5-flash\",\n",
        "                    \"google/gemini-2.5-flash-lite\",\n",
        "                    \"google/gemini-2.5-pro\",\n",
        "\n",
        "                    \"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "                    \"amazon/nova-2-lite-v1:free\",\n",
        "                    \"mistralai/mistral-7b-instruct:free\",\n",
        "                    \"kwaipilot/kat-coder-pro:free\",\n",
        "                    \"tngtech/deepseek-r1t2-chimera:free\",\n",
        "                ],\n",
        "                value=DEFAULT_MODEL_ID,\n",
        "                allow_custom_value=True,\n",
        "            )\n",
        "\n",
        "            temperature_inp = gr.Slider(\n",
        "                label=\"Temperature (0.0 = mazāka variācija, 1.0 = lielāka variācija)\",\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.2,\n",
        "                step=0.05,\n",
        "            )\n",
        "\n",
        "            zip_input = gr.File(label=\"ZIP ar LKP skrejlapu .txt failiem\")\n",
        "            build_btn = gr.Button(\"Izveidot indeksu\")\n",
        "            build_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            top_k_inp = gr.Slider(\n",
        "                label=\"Cik fragmentus izmantot (top_k)?\",\n",
        "                minimum=1,\n",
        "                maximum=30,\n",
        "                value=12,\n",
        "                step=1,\n",
        "            )\n",
        "            preview_chars_inp = gr.Slider(\n",
        "                label=\"Cik simbolus rādīt katrā fragmenta preview?\",\n",
        "                minimum=50,\n",
        "                maximum=1000,\n",
        "                value=300,\n",
        "                step=50,\n",
        "            )\n",
        "\n",
        "            min_score_inp = gr.Slider(\n",
        "                label=\"Minimālais līdzības score (0 = izslēgts)\",\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.0,\n",
        "                step=0.01,\n",
        "            )\n",
        "\n",
        "            show_full_chunks_box = gr.Checkbox(\n",
        "                label=\"Rādīt pilnus fragmentus (nevis tikai preview)\",\n",
        "                value=False,\n",
        "            )\n",
        "\n",
        "            # Filtri\n",
        "            date_from_box = gr.Textbox(\n",
        "                label=\"Datums no (YYYY, YYYY-MM vai YYYY-MM-DD, tukšs – nav filtra)\",\n",
        "                placeholder=\"piem., 1934-01\",\n",
        "            )\n",
        "            date_to_box = gr.Textbox(\n",
        "                label=\"Datums līdz (YYYY, YYYY-MM vai YYYY-MM-DD, tukšs – nav filtra)\",\n",
        "                placeholder=\"piem., 1936-12\",\n",
        "            )\n",
        "\n",
        "            print_run_min_box = gr.Number(\n",
        "                label=\"Tirāža no (>=, tukšs – nav filtra)\",\n",
        "                value=None,\n",
        "                precision=0,\n",
        "            )\n",
        "            print_run_max_box = gr.Number(\n",
        "                label=\"Tirāža līdz (<=, tukšs – nav filtra)\",\n",
        "                value=None,\n",
        "                precision=0,\n",
        "            )\n",
        "\n",
        "            include_unk_print_run_box = gr.Checkbox(\n",
        "                label=\"Iekļaut skrejlapas ar nezināmu tirāžu (unk), ja ir tirāžas filtrs\",\n",
        "                value=True,\n",
        "            )\n",
        "\n",
        "            org_custom_box = gr.Textbox(\n",
        "                label=\"Papildu organizācijas filtrs (brīvs teksts, pēc apakšvirknes)\",\n",
        "                placeholder=\"piem., LKP CK, Rīgas komiteja, Sarkanā palīdzība, VEF, Daugavpils\",\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            question_box = gr.Textbox(\n",
        "                label=\"Jautājums par Latvijas komunistisko organizāciju skrejlapu korpusu\",\n",
        "                lines=3,\n",
        "                placeholder=(\n",
        "                    \"Piemēram: Nosauc, kuriem komunistiem piesprieda nāvessodu \"\n",
        "                    \"Ulmaņa režīma laikā!\"\n",
        "                ),\n",
        "            )\n",
        "            ask_btn = gr.Button(\"Uzdot jautājumu\")\n",
        "            retrieve_btn = gr.Button(\"Rādīt tikai fragmentus (bez LLM)\")\n",
        "\n",
        "            answer_out = gr.Markdown(label=\"Atbilde\")\n",
        "            chunks_out = gr.Textbox(\n",
        "                label=\"Izmantotie fragmenti (preview vai pilni)\",\n",
        "                lines=20,\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                log_btn = gr.Button(\"Izveidot и lejupielādēt žurnālu (JSON)\")\n",
        "                log_file_out = gr.File(\n",
        "                    label=\"Žurnāla fails (lkp_rag_log.json)\",\n",
        "                    interactive=False,\n",
        "                )\n",
        "\n",
        "    build_btn.click(\n",
        "        fn=build_rag_from_zip_gui,\n",
        "        inputs=[zip_input],\n",
        "        outputs=[build_status, chunks_out],\n",
        "    )\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=qa_on_corpus_gui,\n",
        "        inputs=[\n",
        "            api_key_box,\n",
        "            question_box,\n",
        "            top_k_inp,\n",
        "            preview_chars_inp,\n",
        "            min_score_inp,\n",
        "            model_choice_box,\n",
        "            date_from_box,\n",
        "            date_to_box,\n",
        "            print_run_min_box,\n",
        "            print_run_max_box,\n",
        "            org_custom_box,\n",
        "            include_unk_print_run_box,\n",
        "            show_full_chunks_box,\n",
        "            temperature_inp,\n",
        "        ],\n",
        "        outputs=[answer_out, chunks_out],\n",
        "    )\n",
        "\n",
        "    retrieve_btn.click(\n",
        "        fn=retrieve_only_gui,\n",
        "        inputs=[\n",
        "            question_box,\n",
        "            top_k_inp,\n",
        "            preview_chars_inp,\n",
        "            min_score_inp,\n",
        "            date_from_box,\n",
        "            date_to_box,\n",
        "            print_run_min_box,\n",
        "            print_run_max_box,\n",
        "            org_custom_box,\n",
        "            include_unk_print_run_box,\n",
        "            show_full_chunks_box,\n",
        "        ],\n",
        "        outputs=[chunks_out],\n",
        "    )\n",
        "\n",
        "    log_btn.click(\n",
        "        fn=get_log_file_for_gui,\n",
        "        inputs=[],\n",
        "        outputs=[log_file_out],\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gui.launch(inbrowser=True)\n"
      ]
    }
  ]
}